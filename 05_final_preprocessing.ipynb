{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# CIC-IDS2017 EDA - Notebook 5: Final Preprocessing Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Preprocessing Plan: 2025-12-23 19:35:33\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# IMPORTS\n",
    "# =============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(f\"Final Preprocessing Plan: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "load-eda-outputs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: 283,138 rows × 87 columns\n",
      "✓ Phase 3: Transform plan\n",
      "✓ Phase 3: Data quality issues\n",
      "✓ Phase 5: Feature ranking (65 features)\n",
      "✓ Phase 5: Target stats\n",
      "✓ Phase 6: High correlation pairs (55 pairs)\n",
      "✓ Phase 7: Semantic summary\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOAD ALL EDA OUTPUTS\n",
    "# =============================================================================\n",
    "\n",
    "OUTPUT_DIR = \"eda_outputs/\"\n",
    "\n",
    "# Load global sample\n",
    "df = pd.read_csv(os.path.join(OUTPUT_DIR, 'global_sample.csv'), low_memory=False)\n",
    "print(f\"Sample: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
    "\n",
    "# Load Phase 3 outputs\n",
    "with open(os.path.join(OUTPUT_DIR, 'preliminary_transform_plan.json'), 'r') as f:\n",
    "    transform_plan = json.load(f)\n",
    "print(\"✓ Phase 3: Transform plan\")\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIR, 'data_quality_issues.json'), 'r') as f:\n",
    "    data_quality = json.load(f)\n",
    "print(\"✓ Phase 3: Data quality issues\")\n",
    "\n",
    "# Load Phase 5 outputs\n",
    "df_ranking = pd.read_csv(os.path.join(OUTPUT_DIR, 'feature_ranking.csv'), index_col=0)\n",
    "print(f\"✓ Phase 5: Feature ranking ({len(df_ranking)} features)\")\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIR, 'target_analysis_stats.json'), 'r') as f:\n",
    "    target_stats = json.load(f)\n",
    "print(\"✓ Phase 5: Target stats\")\n",
    "\n",
    "# Load Phase 6-7 outputs\n",
    "df_high_corr = pd.read_csv(os.path.join(OUTPUT_DIR, 'high_correlation_pairs.csv'))\n",
    "print(f\"✓ Phase 6: High correlation pairs ({len(df_high_corr)} pairs)\")\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIR, 'phase67_summary.json'), 'r') as f:\n",
    "    phase67_summary = json.load(f)\n",
    "print(\"✓ Phase 7: Semantic summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inventory-header",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Final Feature Inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "feature-inventory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FINAL FEATURE INVENTORY\n",
      "======================================================================\n",
      "\n",
      "--- Column Categories ---\n",
      "\n",
      "Category                            Count\n",
      "------------------------------------------\n",
      "Total columns                          87\n",
      "Identifiers                             5\n",
      "Target columns                          1\n",
      "Derived targets                         2\n",
      "DROP (constant)                        12\n",
      "DROP (redundant)                       20\n",
      "Categorical                             2\n",
      "Numerical (usable)                     45\n",
      "\n",
      "Verification: 87 accounted vs 87 total\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FINAL FEATURE INVENTORY\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"FINAL FEATURE INVENTORY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# All columns\n",
    "all_columns = set(df.columns)\n",
    "\n",
    "# Identifiers (never use as features)\n",
    "IDENTIFIERS = {'Flow ID', 'Source IP', 'Destination IP', 'Timestamp', 'Source_File'}\n",
    "\n",
    "# Target columns\n",
    "TARGET_COLS = {'Label'}\n",
    "\n",
    "# Derived targets (created during EDA)\n",
    "DERIVED_TARGETS = {'Is_Attack', 'Attack_Family'}\n",
    "\n",
    "# Features to DROP - Phase 3 (constant/zero-variance)\n",
    "DROP_CONSTANT = set(transform_plan.get('drop', []))\n",
    "\n",
    "# Features to DROP - Phase 6 (redundant, r >= 0.99)\n",
    "DROP_REDUNDANT = set(phase67_summary['correlation_analysis']['features_to_drop_redundant'])\n",
    "\n",
    "# All features to drop\n",
    "DROP_ALL = DROP_CONSTANT | DROP_REDUNDANT\n",
    "\n",
    "# Categorical features (special handling)\n",
    "CATEGORICAL = {'Protocol', 'Destination Port'}\n",
    "\n",
    "# Numerical features (usable)\n",
    "NUMERICAL_USABLE = []\n",
    "for col in df.columns:\n",
    "    if col in IDENTIFIERS | TARGET_COLS | DERIVED_TARGETS | DROP_ALL | CATEGORICAL:\n",
    "        continue\n",
    "    if df[col].dtype in ['int64', 'int32', 'float64', 'float32']:\n",
    "        NUMERICAL_USABLE.append(col)\n",
    "\n",
    "NUMERICAL_USABLE = sorted(NUMERICAL_USABLE)\n",
    "\n",
    "print(f\"\\n--- Column Categories ---\\n\")\n",
    "print(f\"{'Category':<30} {'Count':>10}\")\n",
    "print(\"-\" * 42)\n",
    "print(f\"{'Total columns':<30} {len(all_columns):>10}\")\n",
    "print(f\"{'Identifiers':<30} {len(IDENTIFIERS & all_columns):>10}\")\n",
    "print(f\"{'Target columns':<30} {len(TARGET_COLS):>10}\")\n",
    "print(f\"{'Derived targets':<30} {len(DERIVED_TARGETS & all_columns):>10}\")\n",
    "print(f\"{'DROP (constant)':<30} {len(DROP_CONSTANT):>10}\")\n",
    "print(f\"{'DROP (redundant)':<30} {len(DROP_REDUNDANT):>10}\")\n",
    "print(f\"{'Categorical':<30} {len(CATEGORICAL):>10}\")\n",
    "print(f\"{'Numerical (usable)':<30} {len(NUMERICAL_USABLE):>10}\")\n",
    "\n",
    "# Verify\n",
    "accounted = len(IDENTIFIERS & all_columns) + len(TARGET_COLS) + len(DERIVED_TARGETS & all_columns) + \\\n",
    "            len(DROP_ALL) + len(CATEGORICAL) + len(NUMERICAL_USABLE)\n",
    "print(f\"\\nVerification: {accounted} accounted vs {len(all_columns)} total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "drop-lists",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Features to DROP (Constant/Zero-Variance) ---\n",
      "  • Bwd Avg Bulk Rate\n",
      "  • Bwd Avg Bytes/Bulk\n",
      "  • Bwd Avg Packets/Bulk\n",
      "  • Bwd PSH Flags\n",
      "  • Bwd URG Flags\n",
      "  • CWE Flag Count\n",
      "  • ECE Flag Count\n",
      "  • Fwd Avg Bulk Rate\n",
      "  • Fwd Avg Bytes/Bulk\n",
      "  • Fwd Avg Packets/Bulk\n",
      "  • Fwd URG Flags\n",
      "  • RST Flag Count\n",
      "\n",
      "--- Features to DROP (Redundant, r ≥ 0.99) ---\n",
      "  • Active Mean\n",
      "  • Active Min\n",
      "  • Average Packet Size\n",
      "  • Avg Bwd Segment Size\n",
      "  • Avg Fwd Segment Size\n",
      "  • Bwd IAT Max\n",
      "  • Bwd IAT Total\n",
      "  • Flow Duration\n",
      "  • Flow IAT Mean\n",
      "  • Fwd IAT Mean\n",
      "  • Fwd IAT Total\n",
      "  • Fwd PSH Flags\n",
      "  • Fwd Packets/s\n",
      "  • Idle Mean\n",
      "  • Idle Min\n",
      "  • Packet Length Std\n",
      "  • Subflow Bwd Bytes\n",
      "  • Subflow Fwd Bytes\n",
      "  • Total Backward Packets\n",
      "  • Total Fwd Packets\n",
      "\n",
      "→ Total to DROP: 32 features\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DROP LISTS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n--- Features to DROP (Constant/Zero-Variance) ---\")\n",
    "for feat in sorted(DROP_CONSTANT):\n",
    "    print(f\"  • {feat}\")\n",
    "\n",
    "print(f\"\\n--- Features to DROP (Redundant, r ≥ 0.99) ---\")\n",
    "for feat in sorted(DROP_REDUNDANT):\n",
    "    print(f\"  • {feat}\")\n",
    "\n",
    "print(f\"\\n→ Total to DROP: {len(DROP_ALL)} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "feature-tiers",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Feature Tiers (by Discriminative Power) ---\n",
      "\n",
      "TOP TIER (score ≥ 0.5): 14 features\n",
      "   1. Init_Win_bytes_backward: 0.945\n",
      "   2. Min Packet Length: 0.753\n",
      "   3. Fwd Packet Length Min: 0.739\n",
      "   4. Bwd Packet Length Std: 0.679\n",
      "   5. Fwd IAT Std: 0.672\n",
      "   6. Bwd Packet Length Min: 0.648\n",
      "   7. Fwd IAT Max: 0.622\n",
      "   8. Bwd Packets/s: 0.589\n",
      "   9. Idle Max: 0.585\n",
      "  10. Fwd Packet Length Mean: 0.556\n",
      "  11. Fwd Packet Length Max: 0.535\n",
      "  12. PSH Flag Count: 0.527\n",
      "  13. Total Length of Fwd Packets: 0.517\n",
      "  14. Flow IAT Std: 0.502\n",
      "\n",
      "MID TIER (0.2 ≤ score < 0.5): 21 features\n",
      "LOW TIER (score < 0.2): 10 features\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FEATURE TIERS (by discriminative power)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n--- Feature Tiers (by Discriminative Power) ---\\n\")\n",
    "\n",
    "# Get tiers from ranking (excluding dropped features)\n",
    "df_ranking_clean = df_ranking[~df_ranking.index.isin(DROP_ALL)]\n",
    "\n",
    "top_tier = df_ranking_clean[df_ranking_clean['combined_score'] >= 0.5].index.tolist()\n",
    "mid_tier = df_ranking_clean[(df_ranking_clean['combined_score'] >= 0.2) & \n",
    "                            (df_ranking_clean['combined_score'] < 0.5)].index.tolist()\n",
    "low_tier = df_ranking_clean[df_ranking_clean['combined_score'] < 0.2].index.tolist()\n",
    "\n",
    "print(f\"TOP TIER (score ≥ 0.5): {len(top_tier)} features\")\n",
    "for i, feat in enumerate(top_tier[:15]):\n",
    "    score = df_ranking_clean.loc[feat, 'combined_score']\n",
    "    print(f\"  {i+1:2}. {feat}: {score:.3f}\")\n",
    "if len(top_tier) > 15:\n",
    "    print(f\"  ... and {len(top_tier) - 15} more\")\n",
    "\n",
    "print(f\"\\nMID TIER (0.2 ≤ score < 0.5): {len(mid_tier)} features\")\n",
    "print(f\"LOW TIER (score < 0.2): {len(low_tier)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preprocessing-header",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Preprocessing Pipeline Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "preprocessing-steps",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PREPROCESSING PIPELINE\n",
      "======================================================================\n",
      "\n",
      "STEP_1_DATA_CLEANING\n",
      "  Remove corrupted rows and handle special values\n",
      "  actions:\n",
      "    • Remove rows with integer overflow (Fwd/Bwd Header Length < -1e9)\n",
      "    • Replace infinite values with 0 (Flow Bytes/s, Flow Packets/s)\n",
      "    • Clip small negative timing values to 0 (Flow IAT Min, Flow Duration)\n",
      "\n",
      "STEP_2_FEATURE_ENGINEERING\n",
      "  Create new features from semantic analysis\n",
      "  new_features:\n",
      "    • has_tcp_handshake: Init_Win_bytes_backward != -1\n",
      "    • is_zero_window: Init_Win_bytes_backward == 0\n",
      "    • is_high_window: Init_Win_bytes_backward > 10000\n",
      "    • is_zero_duration: Flow Duration == 0\n",
      "    • init_win_bwd_clean: max(0, Init_Win_bytes_backward)\n",
      "    ... and 1 more\n",
      "\n",
      "STEP_3_FEATURE_SELECTION\n",
      "  Drop constant and redundant features\n",
      "  drop_constant:\n",
      "    • Fwd Avg Bytes/Bulk\n",
      "    • ECE Flag Count\n",
      "    • Bwd Avg Packets/Bulk\n",
      "    • Bwd Avg Bulk Rate\n",
      "    • CWE Flag Count\n",
      "    ... and 7 more\n",
      "  drop_redundant:\n",
      "    • Total Backward Packets\n",
      "    • Fwd IAT Total\n",
      "    • Subflow Fwd Bytes\n",
      "    • Active Min\n",
      "    • Flow Duration\n",
      "    ... and 15 more\n",
      "\n",
      "STEP_4_CATEGORICAL_ENCODING\n",
      "  Encode categorical features\n",
      "  Protocol: One-hot encoding (3 values: TCP, UDP, HOPOPT)\n",
      "  Destination Port: Frequency encoding (24K unique values)\n",
      "\n",
      "STEP_5_NUMERICAL_SCALING\n",
      "  Scale numerical features\n",
      "  right_skewed: log1p + StandardScaler\n",
      "  normal_like: StandardScaler\n",
      "  sparse: No transform (keep as-is)\n",
      "  binary_engineered: No transform (already 0/1)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PREPROCESSING PIPELINE DEFINITION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PREPROCESSING PIPELINE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "PREPROCESSING_PIPELINE = {\n",
    "    'step_1_data_cleaning': {\n",
    "        'description': 'Remove corrupted rows and handle special values',\n",
    "        'actions': [\n",
    "            'Remove rows with integer overflow (Fwd/Bwd Header Length < -1e9)',\n",
    "            'Replace infinite values with 0 (Flow Bytes/s, Flow Packets/s)',\n",
    "            'Clip small negative timing values to 0 (Flow IAT Min, Flow Duration)',\n",
    "        ]\n",
    "    },\n",
    "    'step_2_feature_engineering': {\n",
    "        'description': 'Create new features from semantic analysis',\n",
    "        'new_features': [\n",
    "            ('has_tcp_handshake', 'Init_Win_bytes_backward != -1'),\n",
    "            ('is_zero_window', 'Init_Win_bytes_backward == 0'),\n",
    "            ('is_high_window', 'Init_Win_bytes_backward > 10000'),\n",
    "            ('is_zero_duration', 'Flow Duration == 0'),\n",
    "            ('init_win_bwd_clean', 'max(0, Init_Win_bytes_backward)'),\n",
    "            ('init_win_fwd_clean', 'max(0, Init_Win_bytes_forward)'),\n",
    "        ]\n",
    "    },\n",
    "    'step_3_feature_selection': {\n",
    "        'description': 'Drop constant and redundant features',\n",
    "        'drop_constant': list(DROP_CONSTANT),\n",
    "        'drop_redundant': list(DROP_REDUNDANT),\n",
    "    },\n",
    "    'step_4_categorical_encoding': {\n",
    "        'description': 'Encode categorical features',\n",
    "        'Protocol': 'One-hot encoding (3 values: TCP, UDP, HOPOPT)',\n",
    "        'Destination Port': 'Frequency encoding (24K unique values)',\n",
    "    },\n",
    "    'step_5_numerical_scaling': {\n",
    "        'description': 'Scale numerical features',\n",
    "        'right_skewed': 'log1p + StandardScaler',\n",
    "        'normal_like': 'StandardScaler',\n",
    "        'sparse': 'No transform (keep as-is)',\n",
    "        'binary_engineered': 'No transform (already 0/1)',\n",
    "    },\n",
    "}\n",
    "\n",
    "for step, details in PREPROCESSING_PIPELINE.items():\n",
    "    print(f\"\\n{step.upper()}\")\n",
    "    print(f\"  {details['description']}\")\n",
    "    for key, value in details.items():\n",
    "        if key == 'description':\n",
    "            continue\n",
    "        if isinstance(value, list):\n",
    "            print(f\"  {key}:\")\n",
    "            for item in value[:5]:\n",
    "                if isinstance(item, tuple):\n",
    "                    print(f\"    • {item[0]}: {item[1]}\")\n",
    "                else:\n",
    "                    print(f\"    • {item}\")\n",
    "            if len(value) > 5:\n",
    "                print(f\"    ... and {len(value) - 5} more\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "feature-groups",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Feature Groups for Scaling ---\n",
      "\n",
      "Log1p + StandardScaler: 27 features\n",
      "No transform (sparse/binary): 18 features\n",
      "Binary (engineered): 4 features\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FEATURE GROUPS FOR SCALING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n--- Feature Groups for Scaling ---\\n\")\n",
    "\n",
    "# Get from Phase 3 transform plan\n",
    "SPARSE_FEATURES = set(transform_plan.get('sparse', []))\n",
    "PENDING_FEATURES = set(transform_plan.get('pending_phase7', []))\n",
    "\n",
    "# Features that need log transform (right-skewed)\n",
    "# Most network features are right-skewed\n",
    "LOG_TRANSFORM = []\n",
    "STANDARD_SCALE = []\n",
    "NO_TRANSFORM = []\n",
    "\n",
    "for feat in NUMERICAL_USABLE:\n",
    "    if feat in SPARSE_FEATURES:\n",
    "        NO_TRANSFORM.append(feat)\n",
    "    elif feat in PENDING_FEATURES:\n",
    "        # These will be handled by feature engineering\n",
    "        NO_TRANSFORM.append(feat)\n",
    "    elif 'Flag' in feat or 'Count' in feat:\n",
    "        # Count features are sparse\n",
    "        NO_TRANSFORM.append(feat)\n",
    "    else:\n",
    "        # Most continuous features are right-skewed\n",
    "        LOG_TRANSFORM.append(feat)\n",
    "\n",
    "print(f\"Log1p + StandardScaler: {len(LOG_TRANSFORM)} features\")\n",
    "print(f\"No transform (sparse/binary): {len(NO_TRANSFORM)} features\")\n",
    "\n",
    "# Binary features from engineering\n",
    "BINARY_ENGINEERED = [\n",
    "    'has_tcp_handshake',\n",
    "    'is_zero_window',\n",
    "    'is_high_window',\n",
    "    'is_zero_duration',\n",
    "]\n",
    "\n",
    "print(f\"Binary (engineered): {len(BINARY_ENGINEERED)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "implementation-header",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Feature Engineering Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "implement-engineering",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FEATURE ENGINEERING IMPLEMENTATION\n",
      "======================================================================\n",
      "\n",
      "--- Step 1: Data Cleaning ---\n",
      "  Removed 1 rows with integer overflow\n",
      "  Replaced 150 inf values in Flow Bytes/s\n",
      "  Replaced 275 inf values in Flow Packets/s\n",
      "  Clipped 300 negative values in Flow IAT Min\n",
      "  Clipped 11 negative values in Flow Duration\n",
      "\n",
      "  Dataset after cleaning: 283,137 rows\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# IMPLEMENT FEATURE ENGINEERING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"FEATURE ENGINEERING IMPLEMENTATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "df_processed = df.copy()\n",
    "\n",
    "# Step 1: Data Cleaning\n",
    "print(\"\\n--- Step 1: Data Cleaning ---\")\n",
    "\n",
    "# Remove rows with integer overflow\n",
    "overflow_mask = (df_processed['Fwd Header Length'] < -1e9) | \\\n",
    "                (df_processed['Bwd Header Length'] < -1e9)\n",
    "n_overflow = overflow_mask.sum()\n",
    "df_processed = df_processed[~overflow_mask].copy()\n",
    "print(f\"  Removed {n_overflow} rows with integer overflow\")\n",
    "\n",
    "# Replace infinite values\n",
    "for col in ['Flow Bytes/s', 'Flow Packets/s']:\n",
    "    if col in df_processed.columns:\n",
    "        n_inf = np.isinf(df_processed[col]).sum()\n",
    "        df_processed[col] = df_processed[col].replace([np.inf, -np.inf], 0)\n",
    "        print(f\"  Replaced {n_inf} inf values in {col}\")\n",
    "\n",
    "# Clip small negative timing values\n",
    "timing_cols = ['Flow IAT Min', 'Flow Duration']\n",
    "for col in timing_cols:\n",
    "    if col in df_processed.columns:\n",
    "        n_neg = (df_processed[col] < 0).sum()\n",
    "        df_processed[col] = df_processed[col].clip(lower=0)\n",
    "        print(f\"  Clipped {n_neg} negative values in {col}\")\n",
    "\n",
    "print(f\"\\n  Dataset after cleaning: {len(df_processed):,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "create-engineered-features",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 2: Feature Engineering ---\n",
      "  Created: has_tcp_handshake (1 if TCP handshake)\n",
      "  Created: is_zero_window (1 if Init_Win = 0, PortScan signature)\n",
      "  Created: is_high_window (1 if Init_Win > 10000, Web Attack signature)\n",
      "  Created: init_win_bwd_clean (sentinel replaced with 0)\n",
      "  Created: init_win_fwd_clean (sentinel replaced with 0)\n",
      "  Created: is_zero_duration (1 if instantaneous flow)\n",
      "  Created: fwd_bwd_packet_ratio\n",
      "  Created: fwd_bwd_bytes_ratio\n",
      "\n",
      "  New features created: 8\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Create Engineered Features\n",
    "print(\"\\n--- Step 2: Feature Engineering ---\")\n",
    "\n",
    "# From Init_Win_bytes_backward (the #1 discriminator)\n",
    "if 'Init_Win_bytes_backward' in df_processed.columns:\n",
    "    # Binary: TCP handshake occurred\n",
    "    df_processed['has_tcp_handshake'] = (df_processed['Init_Win_bytes_backward'] != -1).astype(int)\n",
    "    print(f\"  Created: has_tcp_handshake (1 if TCP handshake)\")\n",
    "    \n",
    "    # Binary: PortScan signature (zero window)\n",
    "    df_processed['is_zero_window'] = (df_processed['Init_Win_bytes_backward'] == 0).astype(int)\n",
    "    print(f\"  Created: is_zero_window (1 if Init_Win = 0, PortScan signature)\")\n",
    "    \n",
    "    # Binary: Web Attack signature (high window)\n",
    "    df_processed['is_high_window'] = (df_processed['Init_Win_bytes_backward'] > 10000).astype(int)\n",
    "    print(f\"  Created: is_high_window (1 if Init_Win > 10000, Web Attack signature)\")\n",
    "    \n",
    "    # Cleaned version (sentinel replaced with 0)\n",
    "    df_processed['init_win_bwd_clean'] = df_processed['Init_Win_bytes_backward'].clip(lower=0)\n",
    "    print(f\"  Created: init_win_bwd_clean (sentinel replaced with 0)\")\n",
    "\n",
    "# From Init_Win_bytes_forward\n",
    "if 'Init_Win_bytes_forward' in df_processed.columns:\n",
    "    df_processed['init_win_fwd_clean'] = df_processed['Init_Win_bytes_forward'].clip(lower=0)\n",
    "    print(f\"  Created: init_win_fwd_clean (sentinel replaced with 0)\")\n",
    "\n",
    "# From Flow Duration\n",
    "if 'Flow Duration' in df_processed.columns:\n",
    "    df_processed['is_zero_duration'] = (df_processed['Flow Duration'] == 0).astype(int)\n",
    "    print(f\"  Created: is_zero_duration (1 if instantaneous flow)\")\n",
    "\n",
    "# Ratios\n",
    "if 'Total Fwd Packets' in df_processed.columns and 'Total Backward Packets' in df_processed.columns:\n",
    "    df_processed['fwd_bwd_packet_ratio'] = df_processed['Total Fwd Packets'] / \\\n",
    "                                           (df_processed['Total Backward Packets'] + 1)\n",
    "    print(f\"  Created: fwd_bwd_packet_ratio\")\n",
    "\n",
    "if 'Total Length of Fwd Packets' in df_processed.columns and 'Total Length of Bwd Packets' in df_processed.columns:\n",
    "    df_processed['fwd_bwd_bytes_ratio'] = df_processed['Total Length of Fwd Packets'] / \\\n",
    "                                          (df_processed['Total Length of Bwd Packets'] + 1)\n",
    "    print(f\"  Created: fwd_bwd_bytes_ratio\")\n",
    "\n",
    "print(f\"\\n  New features created: 8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "verify-engineering",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Verify Engineered Features ---\n",
      "\n",
      "has_tcp_handshake: 49.0% = 1\n",
      "is_zero_window: 9.6% = 1\n",
      "is_high_window: 5.2% = 1\n",
      "is_zero_duration: 0.1% = 1\n",
      "init_win_bwd_clean: 197217.0% = 1\n",
      "init_win_fwd_clean: 702987.1% = 1\n",
      "fwd_bwd_packet_ratio: min=0.01, median=0.67, max=741.00\n",
      "fwd_bwd_bytes_ratio: min=0.00, median=0.34, max=169105.00\n"
     ]
    }
   ],
   "source": [
    "# Verify engineered features\n",
    "print(\"\\n--- Verify Engineered Features ---\\n\")\n",
    "\n",
    "engineered = ['has_tcp_handshake', 'is_zero_window', 'is_high_window', \n",
    "              'is_zero_duration', 'init_win_bwd_clean', 'init_win_fwd_clean',\n",
    "              'fwd_bwd_packet_ratio', 'fwd_bwd_bytes_ratio']\n",
    "\n",
    "for feat in engineered:\n",
    "    if feat in df_processed.columns:\n",
    "        if df_processed[feat].dtype in ['int64', 'int32']:\n",
    "            # Binary feature\n",
    "            pct_1 = df_processed[feat].mean() * 100\n",
    "            print(f\"{feat}: {pct_1:.1f}% = 1\")\n",
    "        else:\n",
    "            # Continuous feature\n",
    "            print(f\"{feat}: min={df_processed[feat].min():.2f}, median={df_processed[feat].median():.2f}, max={df_processed[feat].max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "validate-attack-signatures",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Validate Attack Signatures ---\n",
      "\n",
      "PortScan (n=15,893):\n",
      "  is_zero_window = 1: 99.3%\n",
      "  has_tcp_handshake = 1: 99.9%\n",
      "\n",
      "Web Attacks (n=237):\n",
      "  is_high_window = 1: 78.5%\n",
      "\n",
      "BENIGN (n=227,310):\n",
      "  is_zero_window = 1: 4.8%\n",
      "  is_high_window = 1: 6.3%\n",
      "  has_tcp_handshake = 1: 41.9%\n"
     ]
    }
   ],
   "source": [
    "# Validate attack signatures in engineered features\n",
    "print(\"\\n--- Validate Attack Signatures ---\\n\")\n",
    "\n",
    "if 'Label' in df_processed.columns:\n",
    "    # PortScan should have high is_zero_window\n",
    "    portscan = df_processed[df_processed['Label'] == 'PortScan']\n",
    "    print(f\"PortScan (n={len(portscan):,}):\")\n",
    "    print(f\"  is_zero_window = 1: {portscan['is_zero_window'].mean()*100:.1f}%\")\n",
    "    print(f\"  has_tcp_handshake = 1: {portscan['has_tcp_handshake'].mean()*100:.1f}%\")\n",
    "    \n",
    "    # Web Attacks should have high is_high_window\n",
    "    web_attack = df_processed[df_processed['Label'].str.contains('Web Attack', na=False)]\n",
    "    print(f\"\\nWeb Attacks (n={len(web_attack):,}):\")\n",
    "    print(f\"  is_high_window = 1: {web_attack['is_high_window'].mean()*100:.1f}%\")\n",
    "    \n",
    "    # BENIGN baseline\n",
    "    benign = df_processed[df_processed['Label'] == 'BENIGN']\n",
    "    print(f\"\\nBENIGN (n={len(benign):,}):\")\n",
    "    print(f\"  is_zero_window = 1: {benign['is_zero_window'].mean()*100:.1f}%\")\n",
    "    print(f\"  is_high_window = 1: {benign['is_high_window'].mean()*100:.1f}%\")\n",
    "    print(f\"  has_tcp_handshake = 1: {benign['has_tcp_handshake'].mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-header",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Train/Test Split Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "split-strategy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TRAIN/TEST SPLIT STRATEGY\n",
      "======================================================================\n",
      "\n",
      "--- Class Distribution ---\n",
      "\n",
      "Binary (BENIGN vs ATTACK):\n",
      "  BENIGN: 80.3%\n",
      "  ATTACK: 19.7%\n",
      "  Imbalance ratio: 4.07:1\n",
      "\n",
      "Multi-class (15 classes):\n",
      "  Max/Min ratio: 20664.6:1\n",
      "  Rare classes (<1%): 11\n",
      "\n",
      "--- Recommended Strategy ---\n",
      "\n",
      "1. STRATIFIED SPLIT:\n",
      "   - Use stratified train/test split (80/20)\n",
      "   - Preserve class proportions in both sets\n",
      "   - For multi-class: stratify by Label (all 15 classes)\n",
      "\n",
      "2. CROSS-VALIDATION:\n",
      "   - Use Stratified K-Fold (k=5)\n",
      "   - Each fold maintains class proportions\n",
      "\n",
      "3. HANDLING RARE CLASSES:\n",
      "   - For training: Consider SMOTE on rare classes\n",
      "   - For evaluation: Use macro F1 (treats all classes equally)\n",
      "   - Alternative: Hierarchical classification\n",
      "     Step 1: Binary (BENIGN vs ATTACK)\n",
      "     Step 2: Family (8 families)\n",
      "     Step 3: Specific (within family)\n",
      "\n",
      "4. METRICS:\n",
      "   Binary: F1, Precision, Recall, ROC-AUC, PR-AUC\n",
      "   Multi-class: Macro F1, Weighted F1, Confusion Matrix\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# TRAIN/TEST SPLIT STRATEGY\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TRAIN/TEST SPLIT STRATEGY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Class distribution\n",
    "print(\"\\n--- Class Distribution ---\")\n",
    "binary_dist = target_stats['binary']\n",
    "print(f\"\\nBinary (BENIGN vs ATTACK):\")\n",
    "print(f\"  BENIGN: {binary_dist['benign_pct']:.1f}%\")\n",
    "print(f\"  ATTACK: {binary_dist['attack_pct']:.1f}%\")\n",
    "print(f\"  Imbalance ratio: {binary_dist['imbalance_ratio']:.2f}:1\")\n",
    "\n",
    "multi_stats = target_stats['multiclass']\n",
    "print(f\"\\nMulti-class ({multi_stats['n_classes']} classes):\")\n",
    "print(f\"  Max/Min ratio: {multi_stats['max_min_ratio']:.1f}:1\")\n",
    "print(f\"  Rare classes (<1%): {len(multi_stats['rare_classes'])}\")\n",
    "\n",
    "print(\"\\n--- Recommended Strategy ---\")\n",
    "print(\"\"\"\n",
    "1. STRATIFIED SPLIT:\n",
    "   - Use stratified train/test split (80/20)\n",
    "   - Preserve class proportions in both sets\n",
    "   - For multi-class: stratify by Label (all 15 classes)\n",
    "\n",
    "2. CROSS-VALIDATION:\n",
    "   - Use Stratified K-Fold (k=5)\n",
    "   - Each fold maintains class proportions\n",
    "\n",
    "3. HANDLING RARE CLASSES:\n",
    "   - For training: Consider SMOTE on rare classes\n",
    "   - For evaluation: Use macro F1 (treats all classes equally)\n",
    "   - Alternative: Hierarchical classification\n",
    "     Step 1: Binary (BENIGN vs ATTACK)\n",
    "     Step 2: Family (8 families)\n",
    "     Step 3: Specific (within family)\n",
    "\n",
    "4. METRICS:\n",
    "   Binary: F1, Precision, Recall, ROC-AUC, PR-AUC\n",
    "   Multi-class: Macro F1, Weighted F1, Confusion Matrix\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "implement-split",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final feature count: 53\n",
      "\n",
      "--- Split Results ---\n",
      "Training set: 226,509 samples\n",
      "Test set: 56,628 samples\n",
      "\n",
      "--- Binary Distribution ---\n",
      "Train - ATTACK: 19.7%\n",
      "Test  - ATTACK: 19.7%\n",
      "\n",
      "--- Multi-class Distribution (Top 5) ---\n",
      "BENIGN: Train=80.3%, Test=80.3%\n",
      "DoS Hulk: Train=8.2%, Test=8.2%\n",
      "PortScan: Train=5.6%, Test=5.6%\n",
      "DDoS: Train=4.5%, Test=4.5%\n",
      "DoS GoldenEye: Train=0.4%, Test=0.4%\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# IMPLEMENT TRAIN/TEST SPLIT\n",
    "# =============================================================================\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create Is_Attack if not exists\n",
    "if 'Is_Attack' not in df_processed.columns:\n",
    "    df_processed['Is_Attack'] = (df_processed['Label'] != 'BENIGN').astype(int)\n",
    "\n",
    "# Define feature columns (final list)\n",
    "FINAL_FEATURES = [col for col in df_processed.columns \n",
    "                  if col not in IDENTIFIERS | TARGET_COLS | DERIVED_TARGETS | DROP_ALL\n",
    "                  and df_processed[col].dtype in ['int64', 'int32', 'float64', 'float32']]\n",
    "\n",
    "# Add categorical if encoded\n",
    "# (For now, exclude - will encode during modeling)\n",
    "FINAL_FEATURES = [f for f in FINAL_FEATURES if f not in CATEGORICAL]\n",
    "\n",
    "print(f\"Final feature count: {len(FINAL_FEATURES)}\")\n",
    "\n",
    "# Prepare X and y\n",
    "X = df_processed[FINAL_FEATURES]\n",
    "y_binary = df_processed['Is_Attack']\n",
    "y_multi = df_processed['Label']\n",
    "\n",
    "# Stratified split\n",
    "X_train, X_test, y_train_binary, y_test_binary, y_train_multi, y_test_multi = \\\n",
    "    train_test_split(X, y_binary, y_multi, test_size=0.2, random_state=42, stratify=y_multi)\n",
    "\n",
    "print(f\"\\n--- Split Results ---\")\n",
    "print(f\"Training set: {len(X_train):,} samples\")\n",
    "print(f\"Test set: {len(X_test):,} samples\")\n",
    "\n",
    "# Verify stratification\n",
    "print(f\"\\n--- Binary Distribution ---\")\n",
    "print(f\"Train - ATTACK: {y_train_binary.mean()*100:.1f}%\")\n",
    "print(f\"Test  - ATTACK: {y_test_binary.mean()*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n--- Multi-class Distribution (Top 5) ---\")\n",
    "train_dist = y_train_multi.value_counts(normalize=True).head(5) * 100\n",
    "test_dist = y_test_multi.value_counts(normalize=True).head(5) * 100\n",
    "\n",
    "for label in train_dist.index:\n",
    "    print(f\"{label}: Train={train_dist[label]:.1f}%, Test={test_dist[label]:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export-header",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. Export Preprocessing Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "export-artifacts",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXPORT PREPROCESSING ARTIFACTS\n",
      "======================================================================\n",
      "✓ Saved: feature_lists.json\n",
      "✓ Saved: preprocessing_config.json\n",
      "✓ Saved: sample_processed.csv (283,137 rows)\n",
      "✓ Saved: train_set.csv (226,509 rows)\n",
      "✓ Saved: test_set.csv (56,628 rows)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EXPORT PREPROCESSING ARTIFACTS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EXPORT PREPROCESSING ARTIFACTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create artifacts directory\n",
    "ARTIFACTS_DIR = os.path.join(OUTPUT_DIR, 'preprocessing_artifacts')\n",
    "os.makedirs(ARTIFACTS_DIR, exist_ok=True)\n",
    "\n",
    "# 1. Feature lists\n",
    "feature_lists = {\n",
    "    'final_features': FINAL_FEATURES,\n",
    "    'drop_constant': list(DROP_CONSTANT),\n",
    "    'drop_redundant': list(DROP_REDUNDANT),\n",
    "    'top_tier': top_tier,\n",
    "    'mid_tier': mid_tier,\n",
    "    'low_tier': low_tier,\n",
    "    'log_transform': LOG_TRANSFORM,\n",
    "    'no_transform': NO_TRANSFORM,\n",
    "    'binary_engineered': BINARY_ENGINEERED,\n",
    "    'categorical': list(CATEGORICAL),\n",
    "    'identifiers': list(IDENTIFIERS),\n",
    "}\n",
    "\n",
    "with open(os.path.join(ARTIFACTS_DIR, 'feature_lists.json'), 'w') as f:\n",
    "    json.dump(feature_lists, f, indent=2)\n",
    "print(\"✓ Saved: feature_lists.json\")\n",
    "\n",
    "# 2. Preprocessing config\n",
    "preprocessing_config = {\n",
    "    'pipeline': PREPROCESSING_PIPELINE,\n",
    "    'class_distribution': {\n",
    "        'binary': target_stats['binary'],\n",
    "        'multiclass': target_stats['multiclass'],\n",
    "    },\n",
    "    'split_params': {\n",
    "        'test_size': 0.2,\n",
    "        'random_state': 42,\n",
    "        'stratify': 'Label',\n",
    "    },\n",
    "    'feature_engineering': {\n",
    "        'init_win_bwd_sentinel': -1,\n",
    "        'init_win_fwd_sentinel': -1,\n",
    "        'portscan_signature': 'Init_Win_bytes_backward == 0',\n",
    "        'web_attack_signature': 'Init_Win_bytes_backward > 10000',\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(os.path.join(ARTIFACTS_DIR, 'preprocessing_config.json'), 'w') as f:\n",
    "    json.dump(preprocessing_config, f, indent=2, default=str)\n",
    "print(\"✓ Saved: preprocessing_config.json\")\n",
    "\n",
    "# 3. Save processed sample\n",
    "df_processed.to_csv(os.path.join(ARTIFACTS_DIR, 'sample_processed.csv'), index=False)\n",
    "print(f\"✓ Saved: sample_processed.csv ({len(df_processed):,} rows)\")\n",
    "\n",
    "# 4. Save train/test splits\n",
    "train_data = pd.concat([X_train, y_train_binary.rename('Is_Attack'), y_train_multi.rename('Label')], axis=1)\n",
    "test_data = pd.concat([X_test, y_test_binary.rename('Is_Attack'), y_test_multi.rename('Label')], axis=1)\n",
    "\n",
    "train_data.to_csv(os.path.join(ARTIFACTS_DIR, 'train_set.csv'), index=False)\n",
    "test_data.to_csv(os.path.join(ARTIFACTS_DIR, 'test_set.csv'), index=False)\n",
    "print(f\"✓ Saved: train_set.csv ({len(train_data):,} rows)\")\n",
    "print(f\"✓ Saved: test_set.csv ({len(test_data):,} rows)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "final-summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EDA COMPLETE: FINAL SUMMARY\n",
      "======================================================================\n",
      "\n",
      "--- Dataset ---\n",
      "  Original: 2.8M rows × 79 features\n",
      "  Sample: 283,138 rows (10% stratified)\n",
      "  After cleaning: 283,137 rows\n",
      "\n",
      "--- Features ---\n",
      "  Original numerical: 77\n",
      "  Dropped (constant): 12\n",
      "  Dropped (redundant): 20\n",
      "  Engineered (new): 8\n",
      "  Final features: 53\n",
      "\n",
      "--- Class Balance ---\n",
      "  Binary: 4.1:1 (manageable)\n",
      "  Multi-class: 20665:1 (severe, needs handling)\n",
      "\n",
      "--- Top 5 Discriminating Features ---\n",
      "  1. Init_Win_bytes_backward\n",
      "  2. Min Packet Length\n",
      "  3. Fwd Packet Length Min\n",
      "  4. Bwd Packet Length Std\n",
      "  5. Fwd IAT Std\n",
      "\n",
      "--- Key Attack Signatures ---\n",
      "  • PortScan: Init_Win = 0 (99.3%)\n",
      "  • Web Attack: Init_Win > 10,000 (~28,960)\n",
      "  • DoS: Extreme timing values\n",
      "\n",
      "--- Output Artifacts ---\n",
      "  ✓ feature_lists.json: 4.8 KB\n",
      "  ✓ preprocessing_config.json: 4.9 KB\n",
      "  ✓ sample_processed.csv: 131832.4 KB\n",
      "  ✓ test_set.csv: 13887.9 KB\n",
      "  ✓ train_set.csv: 55503.5 KB\n",
      "\n",
      "--- Ready for Modeling ---\n",
      "\n",
      "Next steps:\n",
      "1. Load train_set.csv and test_set.csv\n",
      "2. Apply scaling (log1p + StandardScaler for continuous features)\n",
      "3. Train baseline models:\n",
      "   - Binary: Logistic Regression, Random Forest, XGBoost\n",
      "   - Multi-class: Same with class_weight='balanced'\n",
      "4. Evaluate on test set\n",
      "5. Consider hierarchical classification for rare classes\n",
      "\n",
      "\n",
      "======================================================================\n",
      "EDA Completed: 2025-12-23 19:35:53\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FINAL EDA SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EDA COMPLETE: FINAL SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n--- Dataset ---\")\n",
    "print(f\"  Original: 2.8M rows × 79 features\")\n",
    "print(f\"  Sample: {len(df):,} rows (10% stratified)\")\n",
    "print(f\"  After cleaning: {len(df_processed):,} rows\")\n",
    "\n",
    "print(\"\\n--- Features ---\")\n",
    "print(f\"  Original numerical: 77\")\n",
    "print(f\"  Dropped (constant): {len(DROP_CONSTANT)}\")\n",
    "print(f\"  Dropped (redundant): {len(DROP_REDUNDANT)}\")\n",
    "print(f\"  Engineered (new): 8\")\n",
    "print(f\"  Final features: {len(FINAL_FEATURES)}\")\n",
    "\n",
    "print(\"\\n--- Class Balance ---\")\n",
    "print(f\"  Binary: {binary_dist['imbalance_ratio']:.1f}:1 (manageable)\")\n",
    "print(f\"  Multi-class: {multi_stats['max_min_ratio']:.0f}:1 (severe, needs handling)\")\n",
    "\n",
    "print(\"\\n--- Top 5 Discriminating Features ---\")\n",
    "for i, feat in enumerate(top_tier[:5]):\n",
    "    print(f\"  {i+1}. {feat}\")\n",
    "\n",
    "print(\"\\n--- Key Attack Signatures ---\")\n",
    "print(\"  • PortScan: Init_Win = 0 (99.3%)\")\n",
    "print(\"  • Web Attack: Init_Win > 10,000 (~28,960)\")\n",
    "print(\"  • DoS: Extreme timing values\")\n",
    "\n",
    "print(\"\\n--- Output Artifacts ---\")\n",
    "for f in os.listdir(ARTIFACTS_DIR):\n",
    "    fpath = os.path.join(ARTIFACTS_DIR, f)\n",
    "    size = os.path.getsize(fpath) / 1024\n",
    "    print(f\"  ✓ {f}: {size:.1f} KB\")\n",
    "\n",
    "print(\"\\n--- Ready for Modeling ---\")\n",
    "print(\"\"\"\n",
    "Next steps:\n",
    "1. Load train_set.csv and test_set.csv\n",
    "2. Apply scaling (log1p + StandardScaler for continuous features)\n",
    "3. Train baseline models:\n",
    "   - Binary: Logistic Regression, Random Forest, XGBoost\n",
    "   - Multi-class: Same with class_weight='balanced'\n",
    "4. Evaluate on test set\n",
    "5. Consider hierarchical classification for rare classes\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"EDA Completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
