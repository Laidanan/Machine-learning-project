{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# CIC-IDS2017 EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase0-header",
   "metadata": {},
   "source": [
    "# Phase 0: Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step-0-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 0.1: IMPORTS\n",
    "# =============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import gc\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "# Plot settings\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(f\"Analysis started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step-0-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 0.2: FILE PATHS - UPDATE THIS\n",
    "# =============================================================================\n",
    "\n",
    "DATA_DIR = \"data/\"  # <-- UPDATE THIS PATH\n",
    "\n",
    "# Output directory for EDA artifacts\n",
    "OUTPUT_DIR = \"eda_outputs/\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Find all CSV files\n",
    "FILE_PATHS = sorted(glob.glob(os.path.join(DATA_DIR, \"*.csv\")))\n",
    "\n",
    "if not FILE_PATHS:\n",
    "    raise FileNotFoundError(f\"No CSV files found in {DATA_DIR}. Please update DATA_DIR.\")\n",
    "\n",
    "print(f\"Found {len(FILE_PATHS)} CSV files:\")\n",
    "for fp in FILE_PATHS:\n",
    "    print(f\"  - {os.path.basename(fp)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step-0-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 0.3: LABEL NORMALIZATION FUNCTION\n",
    "# =============================================================================\n",
    "# CIC-IDS2017 has inconsistent label encoding across different versions.\n",
    "# This function normalizes all variants to a canonical form.\n",
    "\n",
    "def normalize_label(label):\n",
    "    \"\"\"\n",
    "    Normalize attack labels to handle encoding issues and case variations.\n",
    "    \"\"\"\n",
    "    if pd.isna(label):\n",
    "        return 'Unknown'\n",
    "    \n",
    "    label = str(label).strip()\n",
    "    \n",
    "    # Normalize various dash characters to standard hyphen\n",
    "    # Handles: en-dash (–), em-dash (—), replacement char (�), and others\n",
    "    label = re.sub(r'[–—�\\x96\\u2013\\u2014]', '-', label)\n",
    "    \n",
    "    # Normalize to canonical labels (case-insensitive matching)\n",
    "    label_lower = label.lower()\n",
    "    \n",
    "    # Direct mappings for known variations\n",
    "    label_map = {\n",
    "        'benign': 'BENIGN',\n",
    "        'bot': 'Bot',\n",
    "        'ddos': 'DDoS',\n",
    "        'dos hulk': 'DoS Hulk',\n",
    "        'dos goldeneye': 'DoS GoldenEye',\n",
    "        'dos slowloris': 'DoS Slowloris',\n",
    "        'dos slowhttptest': 'DoS Slowhttptest',\n",
    "        'ftp-patator': 'FTP-Patator',\n",
    "        'ssh-patator': 'SSH-Patator',\n",
    "        'heartbleed': 'Heartbleed',\n",
    "        'infiltration': 'Infiltration',\n",
    "        'portscan': 'PortScan',\n",
    "        'web attack - brute force': 'Web Attack - Brute Force',\n",
    "        'web attack - xss': 'Web Attack - XSS',\n",
    "        'web attack - sql injection': 'Web Attack - Sql Injection',\n",
    "    }\n",
    "    \n",
    "    return label_map.get(label_lower, label)\n",
    "\n",
    "\n",
    "# Test the normalization\n",
    "test_labels = [\n",
    "    'BENIGN', 'benign',\n",
    "    'DoS slowloris', 'DoS Slowloris',\n",
    "    'Web Attack � Brute Force', 'Web Attack – Brute Force', 'Web Attack - Brute Force',\n",
    "    'Web Attack � XSS', 'Web Attack – XSS',\n",
    "    'DoS Slowhttptest', 'DoS slowhttptest'\n",
    "]\n",
    "\n",
    "print(\"Label normalization test:\")\n",
    "for label in test_labels:\n",
    "    print(f\"  '{label}' → '{normalize_label(label)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step-0-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 0.4: ATTACK FAMILY MAPPING (Using Normalized Labels)\n",
    "# =============================================================================\n",
    "\n",
    "FAMILY_MAP = {\n",
    "    # Benign\n",
    "    'BENIGN': 'BENIGN',\n",
    "    \n",
    "    # DoS/DDoS Family\n",
    "    'DoS Hulk': 'DoS',\n",
    "    'DoS GoldenEye': 'DoS',\n",
    "    'DoS Slowloris': 'DoS',\n",
    "    'DoS Slowhttptest': 'DoS',\n",
    "    'DDoS': 'DoS',\n",
    "    \n",
    "    # Brute Force Family\n",
    "    'FTP-Patator': 'BruteForce',\n",
    "    'SSH-Patator': 'BruteForce',\n",
    "    \n",
    "    # Web Attack Family (using normalized dash)\n",
    "    'Web Attack - Brute Force': 'WebAttack',\n",
    "    'Web Attack - XSS': 'WebAttack',\n",
    "    'Web Attack - Sql Injection': 'WebAttack',\n",
    "    \n",
    "    # Probe/Scan Family\n",
    "    'PortScan': 'Probe',\n",
    "    \n",
    "    # Botnet Family\n",
    "    'Bot': 'Botnet',\n",
    "    \n",
    "    # Infiltration Family\n",
    "    'Infiltration': 'Infiltration',\n",
    "    \n",
    "    # Heartbleed\n",
    "    'Heartbleed': 'Heartbleed'\n",
    "}\n",
    "\n",
    "print(\"Attack Family Mapping:\")\n",
    "for label, family in FAMILY_MAP.items():\n",
    "    if label != 'BENIGN':\n",
    "        print(f\"  {label} → {family}\")\n",
    "\n",
    "print(f\"\\nTotal attack types: {len([k for k in FAMILY_MAP if k != 'BENIGN'])}\")\n",
    "print(f\"Attack families: {sorted(set(v for v in FAMILY_MAP.values() if v != 'BENIGN'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step-0-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 0.5: GLOBAL CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "CONFIG = {\n",
    "    # Sampling\n",
    "    'sample_fraction': 0.10,\n",
    "    'min_samples_keep_all': 100,\n",
    "    'random_seed': 42,\n",
    "    \n",
    "    # Thresholds\n",
    "    'high_correlation_threshold': 0.95,\n",
    "    'near_zero_variance_threshold': 0.99,\n",
    "    'imbalance_warning_threshold': 0.95,\n",
    "    \n",
    "    # Column classifications\n",
    "    'identifier_columns': ['Flow ID', 'Source IP', 'Destination IP', 'Timestamp'],\n",
    "    'target_column': 'Label',\n",
    "    'categorical_columns': ['Protocol', 'Destination Port'],\n",
    "    \n",
    "    # Paths\n",
    "    'data_dir': DATA_DIR,\n",
    "    'output_dir': OUTPUT_DIR,\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  - Sample fraction: {CONFIG['sample_fraction']*100}%\")\n",
    "print(f\"  - Random seed: {CONFIG['random_seed']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step-0-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 0.6: HELPER FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def get_file_category(filepath):\n",
    "    \"\"\"Extract category from filename (e.g., 'Botnet' from 'Botnet.csv').\"\"\"\n",
    "    basename = os.path.basename(filepath)\n",
    "    return os.path.splitext(basename)[0]\n",
    "\n",
    "\n",
    "def estimate_memory_mb(df):\n",
    "    \"\"\"Estimate DataFrame memory usage in MB.\"\"\"\n",
    "    return df.memory_usage(deep=True).sum() / (1024 * 1024)\n",
    "\n",
    "\n",
    "def count_infinities(df):\n",
    "    \"\"\"Count infinite values in numeric columns.\"\"\"\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) == 0:\n",
    "        return 0, {}\n",
    "    \n",
    "    inf_counts = {}\n",
    "    total = 0\n",
    "    for col in numeric_cols:\n",
    "        count = np.isinf(df[col]).sum()\n",
    "        if count > 0:\n",
    "            inf_counts[col] = int(count)\n",
    "            total += count\n",
    "    return total, inf_counts\n",
    "\n",
    "\n",
    "def count_negatives(df):\n",
    "    \"\"\"Count negative values in numeric columns.\"\"\"\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    neg_counts = {}\n",
    "    total = 0\n",
    "    for col in numeric_cols:\n",
    "        count = (df[col] < 0).sum()\n",
    "        if count > 0:\n",
    "            neg_counts[col] = int(count)\n",
    "            total += count\n",
    "    return total, neg_counts\n",
    "\n",
    "\n",
    "def clean_column_names(df):\n",
    "    \"\"\"Strip whitespace from column names.\"\"\"\n",
    "    df.columns = df.columns.str.strip()\n",
    "    return df\n",
    "\n",
    "\n",
    "def normalize_labels_in_df(df, label_col='Label'):\n",
    "    \"\"\"Apply label normalization to a DataFrame.\"\"\"\n",
    "    if label_col in df.columns:\n",
    "        df[label_col] = df[label_col].apply(normalize_label)\n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase1-header",
   "metadata": {},
   "source": [
    "---\n",
    "# Phase 1: Per-File Health Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step-1-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 1.1: SCHEMA CONSISTENCY CHECK\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 1.1: SCHEMA CONSISTENCY CHECK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "schemas = {}\n",
    "\n",
    "for filepath in FILE_PATHS:\n",
    "    filename = os.path.basename(filepath)\n",
    "    category = get_file_category(filepath)\n",
    "    \n",
    "    df_header = pd.read_csv(filepath, nrows=0)\n",
    "    df_header = clean_column_names(df_header)\n",
    "    \n",
    "    columns = set(df_header.columns)\n",
    "    schemas[filename] = {\n",
    "        'category': category,\n",
    "        'columns': columns,\n",
    "        'num_columns': len(columns)\n",
    "    }\n",
    "    \n",
    "    print(f\"{filename} ({category}): {len(columns)} columns\")\n",
    "\n",
    "# Check schema consistency\n",
    "reference_file = list(schemas.keys())[0]\n",
    "reference_columns = schemas[reference_file]['columns']\n",
    "\n",
    "print(\"\\n--- Schema Comparison ---\")\n",
    "all_match = True\n",
    "\n",
    "for filename, info in schemas.items():\n",
    "    if info['columns'] != reference_columns:\n",
    "        all_match = False\n",
    "        missing = reference_columns - info['columns']\n",
    "        extra = info['columns'] - reference_columns\n",
    "        if missing:\n",
    "            print(f\"  {filename}: MISSING columns: {missing}\")\n",
    "        if extra:\n",
    "            print(f\"  {filename}: EXTRA columns: {extra}\")\n",
    "\n",
    "if all_match:\n",
    "    print(\"  ✓ All files have identical schemas.\")\n",
    "\n",
    "common_columns = set.intersection(*[info['columns'] for info in schemas.values()])\n",
    "print(f\"\\nCommon columns: {len(common_columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step-1-2-to-1-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEPS 1.2-1.8: PER-FILE HEALTH ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEPS 1.2-1.8: PER-FILE HEALTH ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "file_health_records = []\n",
    "all_label_counts = {}  # Aggregate labels across ALL files\n",
    "\n",
    "for filepath in FILE_PATHS:\n",
    "    filename = os.path.basename(filepath)\n",
    "    category = get_file_category(filepath)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing: {filename} (Category: {category})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(filepath, low_memory=False)\n",
    "        df = clean_column_names(df)\n",
    "        df = normalize_labels_in_df(df, 'Label')  # NORMALIZE LABELS\n",
    "        \n",
    "        # -----------------------------------------------------------------\n",
    "        # 1.2: Shape & Memory\n",
    "        # -----------------------------------------------------------------\n",
    "        n_rows, n_cols = df.shape\n",
    "        memory_mb = estimate_memory_mb(df)\n",
    "        \n",
    "        print(f\"\\n[1.2] Shape & Memory:\")\n",
    "        print(f\"      Rows: {n_rows:,} | Columns: {n_cols} | Memory: {memory_mb:.2f} MB\")\n",
    "        \n",
    "        # -----------------------------------------------------------------\n",
    "        # 1.3: Data Types\n",
    "        # -----------------------------------------------------------------\n",
    "        dtype_counts = df.dtypes.value_counts()\n",
    "        print(f\"\\n[1.3] Data Types:\")\n",
    "        for dtype, count in dtype_counts.items():\n",
    "            print(f\"      {dtype}: {count}\")\n",
    "        \n",
    "        # -----------------------------------------------------------------\n",
    "        # 1.4: Missing Values\n",
    "        # -----------------------------------------------------------------\n",
    "        null_total = df.isnull().sum().sum()\n",
    "        cols_with_nulls = df.isnull().sum()\n",
    "        cols_with_nulls = cols_with_nulls[cols_with_nulls > 0]\n",
    "        \n",
    "        print(f\"\\n[1.4] Missing Values: {null_total:,} total\")\n",
    "        if len(cols_with_nulls) > 0:\n",
    "            for col, count in cols_with_nulls.head(3).items():\n",
    "                print(f\"      - {col}: {count:,} ({count/n_rows*100:.2f}%)\")\n",
    "        \n",
    "        # -----------------------------------------------------------------\n",
    "        # 1.5: Infinity Values\n",
    "        # -----------------------------------------------------------------\n",
    "        inf_total, inf_by_col = count_infinities(df)\n",
    "        print(f\"\\n[1.5] Infinity Values: {inf_total:,} total\")\n",
    "        if inf_total > 0:\n",
    "            for col, count in list(inf_by_col.items())[:3]:\n",
    "                print(f\"      - {col}: {count:,}\")\n",
    "        \n",
    "        # -----------------------------------------------------------------\n",
    "        # 1.6: Negative Values\n",
    "        # -----------------------------------------------------------------\n",
    "        neg_total, neg_by_col = count_negatives(df)\n",
    "        print(f\"\\n[1.6] Negative Values: {neg_total:,} total\")\n",
    "        if neg_total > 0:\n",
    "            for col, count in list(neg_by_col.items())[:3]:\n",
    "                print(f\"      - {col}: {count:,}\")\n",
    "        \n",
    "        # -----------------------------------------------------------------\n",
    "        # 1.7: Duplicates\n",
    "        # -----------------------------------------------------------------\n",
    "        n_duplicates = df.duplicated().sum()\n",
    "        print(f\"\\n[1.7] Duplicates: {n_duplicates:,} ({n_duplicates/n_rows*100:.2f}%)\")\n",
    "        \n",
    "        # -----------------------------------------------------------------\n",
    "        # 1.8: Label Distribution (with aggregation)\n",
    "        # -----------------------------------------------------------------\n",
    "        if 'Label' in df.columns:\n",
    "            label_counts = df['Label'].value_counts()\n",
    "            \n",
    "            print(f\"\\n[1.8] Label Distribution:\")\n",
    "            for label, count in label_counts.items():\n",
    "                pct = count / n_rows * 100\n",
    "                family = FAMILY_MAP.get(label, 'UNMAPPED')\n",
    "                print(f\"      - {label}: {count:,} ({pct:.2f}%) [Family: {family}]\")\n",
    "                \n",
    "                # Aggregate into global counts\n",
    "                all_label_counts[label] = all_label_counts.get(label, 0) + count\n",
    "            \n",
    "            # Check for unmapped labels\n",
    "            unmapped = [l for l in label_counts.index if l not in FAMILY_MAP]\n",
    "            if unmapped:\n",
    "                print(f\"      ⚠ UNMAPPED LABELS: {unmapped}\")\n",
    "            \n",
    "            benign_pct = label_counts.get('BENIGN', 0) / n_rows * 100\n",
    "        else:\n",
    "            benign_pct = 0\n",
    "        \n",
    "        # Store record\n",
    "        file_health_records.append({\n",
    "            'filename': filename,\n",
    "            'category': category,\n",
    "            'n_rows': n_rows,\n",
    "            'n_columns': n_cols,\n",
    "            'memory_mb': round(memory_mb, 2),\n",
    "            'null_total': null_total,\n",
    "            'inf_total': inf_total,\n",
    "            'neg_total': neg_total,\n",
    "            'n_duplicates': n_duplicates,\n",
    "            'n_labels': len(label_counts) if 'Label' in df.columns else 0,\n",
    "            'benign_pct': round(benign_pct, 2)\n",
    "        })\n",
    "        \n",
    "        del df\n",
    "        gc.collect()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Per-file analysis complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "health-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FILE HEALTH REPORT SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FILE HEALTH REPORT SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "df_health = pd.DataFrame(file_health_records)\n",
    "print(\"\\n\")\n",
    "print(df_health.to_string(index=False))\n",
    "\n",
    "print(\"\\n--- Aggregated Statistics ---\")\n",
    "print(f\"Total rows: {df_health['n_rows'].sum():,}\")\n",
    "print(f\"Total memory: {df_health['memory_mb'].sum():.2f} MB\")\n",
    "print(f\"Total nulls: {df_health['null_total'].sum():,}\")\n",
    "print(f\"Total infinities: {df_health['inf_total'].sum():,}\")\n",
    "print(f\"Total negatives: {df_health['neg_total'].sum():,}\")\n",
    "print(f\"Total duplicates: {df_health['n_duplicates'].sum():,}\")\n",
    "\n",
    "# Save\n",
    "health_path = os.path.join(OUTPUT_DIR, 'file_health_report.csv')\n",
    "df_health.to_csv(health_path, index=False)\n",
    "print(f\"\\n✓ Saved to: {health_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "global-label-dist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GLOBAL LABEL DISTRIBUTION (Aggregated from ALL files)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"GLOBAL LABEL DISTRIBUTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Convert to Series and sort\n",
    "label_series = pd.Series(all_label_counts).sort_values(ascending=False)\n",
    "total_samples = label_series.sum()\n",
    "\n",
    "print(f\"\\nTotal samples across all files: {total_samples:,}\")\n",
    "print(f\"Unique labels: {len(label_series)}\")\n",
    "print(\"\\n--- Label Counts ---\")\n",
    "\n",
    "for label, count in label_series.items():\n",
    "    pct = count / total_samples * 100\n",
    "    family = FAMILY_MAP.get(label, 'UNMAPPED')\n",
    "    marker = \"⚠\" if family == 'UNMAPPED' else \"✓\"\n",
    "    print(f\"  {marker} {label}: {count:,} ({pct:.2f}%) → {family}\")\n",
    "\n",
    "# Check for unmapped\n",
    "unmapped_labels = [l for l in label_series.index if l not in FAMILY_MAP]\n",
    "if unmapped_labels:\n",
    "    print(f\"\\n⚠ WARNING: {len(unmapped_labels)} unmapped labels found!\")\n",
    "    print(f\"  You need to add these to FAMILY_MAP: {unmapped_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hierarchical-balance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# HIERARCHICAL BALANCE ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"HIERARCHICAL BALANCE ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Binary balance\n",
    "total_benign = all_label_counts.get('BENIGN', 0)\n",
    "total_attack = total_samples - total_benign\n",
    "\n",
    "print(\"\\n--- Stage 1: Binary Classification ---\")\n",
    "print(f\"  BENIGN: {total_benign:,} ({total_benign/total_samples*100:.2f}%)\")\n",
    "print(f\"  ATTACK: {total_attack:,} ({total_attack/total_samples*100:.2f}%)\")\n",
    "if total_attack > 0:\n",
    "    print(f\"  Imbalance ratio: {total_benign/total_attack:.2f}:1\")\n",
    "\n",
    "# Family balance (attack only)\n",
    "print(\"\\n--- Stage 2: Attack Family Classification ---\")\n",
    "print(\"  (Excluding BENIGN)\")\n",
    "\n",
    "family_counts = {}\n",
    "for label, count in all_label_counts.items():\n",
    "    if label == 'BENIGN':\n",
    "        continue\n",
    "    family = FAMILY_MAP.get(label, 'Unknown')\n",
    "    family_counts[family] = family_counts.get(family, 0) + count\n",
    "\n",
    "family_series = pd.Series(family_counts).sort_values(ascending=False)\n",
    "\n",
    "for family, count in family_series.items():\n",
    "    pct = count / total_attack * 100 if total_attack > 0 else 0\n",
    "    print(f\"  {family}: {count:,} ({pct:.2f}%)\")\n",
    "\n",
    "# Rare attack warnings\n",
    "print(\"\\n--- Rare Attack Warnings (<100 samples) ---\")\n",
    "for label, count in label_series.items():\n",
    "    if label != 'BENIGN' and count < 100:\n",
    "        print(f\"  ⚠ {label}: {count} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualizations",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VISUALIZATIONS\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Plot 1: Binary balance pie\n",
    "axes[0].pie(\n",
    "    [total_benign, total_attack], \n",
    "    labels=['BENIGN', 'ATTACK'],\n",
    "    autopct='%1.1f%%', \n",
    "    colors=['#2ecc71', '#e74c3c'],\n",
    "    startangle=90,\n",
    "    explode=[0, 0.05]\n",
    ")\n",
    "axes[0].set_title('Stage 1: Binary Balance')\n",
    "\n",
    "# Plot 2: All labels bar (log scale)\n",
    "label_series_sorted = label_series.sort_values(ascending=True)\n",
    "colors_bar = ['#2ecc71' if l == 'BENIGN' else '#e74c3c' for l in label_series_sorted.index]\n",
    "label_series_sorted.plot(kind='barh', ax=axes[1], color=colors_bar)\n",
    "axes[1].set_xscale('log')\n",
    "axes[1].set_xlabel('Count (log scale)')\n",
    "axes[1].set_title('All Labels Distribution')\n",
    "\n",
    "# Plot 3: File sizes\n",
    "df_health_sorted = df_health.sort_values('n_rows', ascending=True)\n",
    "df_health_sorted.plot(\n",
    "    kind='barh', \n",
    "    x='category', \n",
    "    y='n_rows', \n",
    "    ax=axes[2],\n",
    "    color='#3498db',\n",
    "    legend=False\n",
    ")\n",
    "axes[2].set_xlabel('Number of Rows')\n",
    "axes[2].set_title('Rows per File')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'data_overview.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Saved to: {os.path.join(OUTPUT_DIR, 'data_overview.png')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase2-header",
   "metadata": {},
   "source": [
    "---\n",
    "# Phase 2: Build Stratified Global Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase2-sampling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# BUILD STRATIFIED GLOBAL SAMPLE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PHASE 2: BUILDING STRATIFIED GLOBAL SAMPLE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "SAMPLE_FRAC = CONFIG['sample_fraction']\n",
    "MIN_KEEP_ALL = CONFIG['min_samples_keep_all']\n",
    "SEED = CONFIG['random_seed']\n",
    "\n",
    "global_samples = []\n",
    "\n",
    "for filepath in FILE_PATHS:\n",
    "    filename = os.path.basename(filepath)\n",
    "    category = get_file_category(filepath)\n",
    "    \n",
    "    print(f\"\\nSampling: {filename}...\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(filepath, low_memory=False)\n",
    "        df = clean_column_names(df)\n",
    "        df = normalize_labels_in_df(df, 'Label')  # NORMALIZE LABELS\n",
    "        original_rows = len(df)\n",
    "        \n",
    "        # Add source file column\n",
    "        df['Source_File'] = category\n",
    "        \n",
    "        # Stratified sampling by Label\n",
    "        if 'Label' in df.columns:\n",
    "            samples_list = []\n",
    "            for label, group in df.groupby('Label'):\n",
    "                if len(group) < MIN_KEEP_ALL:\n",
    "                    # Keep all for rare classes\n",
    "                    samples_list.append(group)\n",
    "                    print(f\"  - {label}: keeping all {len(group)} samples (rare class)\")\n",
    "                else:\n",
    "                    # Sample fraction\n",
    "                    sampled = group.sample(frac=SAMPLE_FRAC, random_state=SEED)\n",
    "                    samples_list.append(sampled)\n",
    "            sample = pd.concat(samples_list, ignore_index=True)\n",
    "        else:\n",
    "            sample = df.sample(frac=SAMPLE_FRAC, random_state=SEED)\n",
    "        \n",
    "        print(f\"  Total: {original_rows:,} → {len(sample):,} ({len(sample)/original_rows*100:.1f}%)\")\n",
    "        global_samples.append(sample)\n",
    "        \n",
    "        del df\n",
    "        gc.collect()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR: {e}\")\n",
    "\n",
    "# Concatenate\n",
    "print(\"\\n--- Merging samples ---\")\n",
    "global_df = pd.concat(global_samples, axis=0, ignore_index=True)\n",
    "print(f\"Global sample shape: {global_df.shape}\")\n",
    "\n",
    "del global_samples\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add-hierarchy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ADD HIERARCHY LABELS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n--- Adding hierarchy labels ---\")\n",
    "\n",
    "# Binary label\n",
    "global_df['Is_Attack'] = (global_df['Label'] != 'BENIGN').astype(int)\n",
    "\n",
    "# Family label\n",
    "global_df['Attack_Family'] = global_df['Label'].map(FAMILY_MAP)\n",
    "\n",
    "# Check for unmapped\n",
    "unmapped_mask = global_df['Attack_Family'].isna()\n",
    "if unmapped_mask.sum() > 0:\n",
    "    unmapped_labels = global_df.loc[unmapped_mask, 'Label'].unique()\n",
    "    print(f\"\\n⚠ WARNING: {unmapped_mask.sum()} rows have unmapped labels!\")\n",
    "    print(f\"  Labels: {unmapped_labels}\")\n",
    "    global_df['Attack_Family'] = global_df['Attack_Family'].fillna('Unknown')\n",
    "\n",
    "# Verify\n",
    "print(\"\\n--- Verification ---\")\n",
    "print(f\"\\nIs_Attack distribution:\")\n",
    "print(global_df['Is_Attack'].value_counts().to_string())\n",
    "\n",
    "print(f\"\\nAttack_Family distribution:\")\n",
    "print(global_df['Attack_Family'].value_counts().to_string())\n",
    "\n",
    "print(f\"\\nLabel distribution (sampled):\")\n",
    "print(global_df['Label'].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "memory-opt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MEMORY OPTIMIZATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n--- Memory Optimization ---\")\n",
    "\n",
    "memory_before = estimate_memory_mb(global_df)\n",
    "print(f\"Before: {memory_before:.2f} MB\")\n",
    "\n",
    "# Downcast numerics\n",
    "for col in global_df.select_dtypes(include=['int64']).columns:\n",
    "    global_df[col] = pd.to_numeric(global_df[col], downcast='integer')\n",
    "\n",
    "for col in global_df.select_dtypes(include=['float64']).columns:\n",
    "    global_df[col] = pd.to_numeric(global_df[col], downcast='float')\n",
    "\n",
    "# Convert strings to category\n",
    "for col in ['Label', 'Attack_Family', 'Source_File']:\n",
    "    if col in global_df.columns:\n",
    "        global_df[col] = global_df[col].astype('category')\n",
    "\n",
    "memory_after = estimate_memory_mb(global_df)\n",
    "print(f\"After: {memory_after:.2f} MB\")\n",
    "print(f\"Reduction: {(1 - memory_after/memory_before)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a12930",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-sample",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SAVE GLOBAL SAMPLE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n--- Saving Global Sample ---\")\n",
    "\n",
    "sample_path = os.path.join(OUTPUT_DIR, 'global_sample.csv')\n",
    "global_df.to_csv(sample_path, index=False)\n",
    "\n",
    "file_size_mb = os.path.getsize(sample_path) / (1024 * 1024)\n",
    "\n",
    "print(f\"✓ Saved to: {sample_path}\")\n",
    "print(f\"  Rows: {len(global_df):,}\")\n",
    "print(f\"  Columns: {global_df.shape[1]}\")\n",
    "print(f\"  File size: {file_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FINAL SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"NOTEBOOK 1 COMPLETE: SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n--- Dataset Overview ---\")\n",
    "print(f\"Files processed: {len(FILE_PATHS)}\")\n",
    "print(f\"Total rows (full): {df_health['n_rows'].sum():,}\")\n",
    "print(f\"Total rows (sample): {len(global_df):,}\")\n",
    "print(f\"Effective sample rate: {len(global_df)/df_health['n_rows'].sum()*100:.1f}%\")\n",
    "\n",
    "print(\"\\n--- Data Quality ---\")\n",
    "print(f\"Nulls: {df_health['null_total'].sum():,}\")\n",
    "print(f\"Infinities: {df_health['inf_total'].sum():,}\")\n",
    "print(f\"Negatives: {df_health['neg_total'].sum():,}\")\n",
    "\n",
    "print(\"\\n--- Hierarchical Balance (Sampled Data) ---\")\n",
    "benign_n = (global_df['Is_Attack'] == 0).sum()\n",
    "attack_n = (global_df['Is_Attack'] == 1).sum()\n",
    "print(f\"Binary: BENIGN={benign_n:,} ({benign_n/len(global_df)*100:.1f}%), ATTACK={attack_n:,} ({attack_n/len(global_df)*100:.1f}%)\")\n",
    "print(f\"Families: {global_df['Attack_Family'].nunique()}\")\n",
    "print(f\"Attack types: {global_df[global_df['Is_Attack']==1]['Label'].nunique()}\")\n",
    "\n",
    "print(\"\\n--- Output Files ---\")\n",
    "for f in os.listdir(OUTPUT_DIR):\n",
    "    fpath = os.path.join(OUTPUT_DIR, f)\n",
    "    size = os.path.getsize(fpath) / 1024  # KB\n",
    "    print(f\"  {f}: {size:.1f} KB\")\n",
    "\n",
    "print(\"\\n--- Next Steps ---\")\n",
    "print(\"  → Run Notebook 02: Feature Analysis\")\n",
    "print(\"  → Run Notebook 03: Target Analysis\")\n",
    "\n",
    "# Save config\n",
    "config_path = os.path.join(OUTPUT_DIR, 'CONFIG.json')\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(CONFIG, f, indent=2)\n",
    "print(f\"\\n✓ Config saved to: {config_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
