{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413d988c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.model_selection import train_test_split\n",
    "import glob\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554248ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# CONFIGURATION\n",
    "# ==========================================\n",
    "file_pattern = \"data/*.csv\" \n",
    "files = glob.glob(file_pattern)\n",
    "files.sort()\n",
    "\n",
    "numeric_stats_list = []\n",
    "categorical_stats_list = []\n",
    "\n",
    "print(f\"--- STARTING SANITIZED UNIVARIATE ANALYSIS ON {len(files)} FILES ---\\n\")\n",
    "\n",
    "for filepath in files:\n",
    "    filename = os.path.basename(filepath)\n",
    "    print(f\"Processing: {filename}...\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(filepath, low_memory=False)\n",
    "        df.columns = df.columns.str.strip()\n",
    "\n",
    "        print(\"\\n--- HEALTH CHECK ---\")\n",
    "        print(\"\\n--- CHECKING DATA TYPES ---\")\n",
    "        # We look for columns that are 'object' (strings) but should be numbers\n",
    "        print(df.dtypes.value_counts())\n",
    "        \n",
    "        print(\"\\n--- MISSING & INFINITE VALUES ---\")\n",
    "        # Standard NULL check\n",
    "        null_counts = df.isnull().sum().sum()\n",
    "        print(f\"Total Null Values: {null_counts}\")\n",
    "        \n",
    "        # CRITICAL FOR NETWORK DATA: Check for Infinity\n",
    "        # Select only numeric columns to avoid errors with string columns\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        inf_counts = np.isinf(df[numeric_cols]).sum().sum()\n",
    "        print(f\"Total Infinite Values (inf/-inf): {inf_counts}\")\n",
    "\n",
    "        print(\"\\n--- DUPLICATES ---\")\n",
    "        duplicates = df.duplicated().sum()\n",
    "        print(f\"Duplicate Rows: {duplicates} ({duplicates/len(df)*100:.2f}%)\")\n",
    "        \n",
    "        # Identify types\n",
    "        cat_cols = ['Label', 'Protocol', 'Destination Port', 'Source Port']\n",
    "        flag_cols = [c for c in df.columns if 'Flag' in c]\n",
    "        cat_cols.extend(flag_cols)\n",
    "        \n",
    "        existing_cat = [c for c in cat_cols if c in df.columns]\n",
    "        existing_num = [c for c in df.columns if c not in existing_cat and pd.api.types.is_numeric_dtype(df[c])]\n",
    "\n",
    "        # ==========================================\n",
    "        # A. NUMERICAL ANALYSIS (Sanitized)\n",
    "        # ==========================================\n",
    "        if existing_num:\n",
    "            for col in existing_num:\n",
    "                series = df[col]\n",
    "                \n",
    "                # 1. HARD STATISTICS (Counts of Dirty Data)\n",
    "                total_rows = len(series)\n",
    "                neg_count = (series < 0).sum()\n",
    "                inf_count = np.isinf(series).sum()\n",
    "                nan_count = series.isnull().sum()\n",
    "                \n",
    "                # 2. SANITIZATION FOR MOMENTS (Mean, Skew, etc.)\n",
    "                # We replace Inf with NaN, then drop NaNs so the math works on valid numbers only\n",
    "                clean_series = series.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "                \n",
    "                if len(clean_series) > 0:\n",
    "                    # Physics Check: For Flow Duration, we might want to exclude negatives from the Mean calc\n",
    "                    # (Optional: depends on if you think -1 is a flag or noise. \n",
    "                    # For now, we include negatives in stats to show the error magnitude)\n",
    "                    \n",
    "                    # Calculated Moments\n",
    "                    mean_val = clean_series.mean()\n",
    "                    median_val = clean_series.median()\n",
    "                    std_val = clean_series.std()\n",
    "                    min_val = clean_series.min()\n",
    "                    max_val = clean_series.max()\n",
    "                    skew_val = clean_series.skew()\n",
    "                    kurt_val = clean_series.kurt()\n",
    "                    \n",
    "                    # Outlier Check (on clean data)\n",
    "                    Q1 = clean_series.quantile(0.25)\n",
    "                    Q3 = clean_series.quantile(0.75)\n",
    "                    IQR = Q3 - Q1\n",
    "                    outliers = ((clean_series < (Q1 - 1.5 * IQR)) | (clean_series > (Q3 + 1.5 * IQR))).sum()\n",
    "                else:\n",
    "                    # If column was 100% Inf or NaN\n",
    "                    mean_val = median_val = std_val = min_val = max_val = skew_val = kurt_val = outliers = 0\n",
    "\n",
    "                numeric_stats_list.append({\n",
    "                    'File': filename,\n",
    "                    'Feature': col,\n",
    "                    'Mean': mean_val,\n",
    "                    'Median': median_val,\n",
    "                    'Std_Dev': std_val,\n",
    "                    'Min': min_val,\n",
    "                    'Max': max_val,\n",
    "                    'Skewness': skew_val,\n",
    "                    'Kurtosis': kurt_val,\n",
    "                    'Neg_Values': neg_count,\n",
    "                    'Inf_Values': inf_count,\n",
    "                    'NaN_Values': nan_count,\n",
    "                    'Outlier_Count': outliers,\n",
    "                    'Total_Rows': total_rows\n",
    "                })\n",
    "\n",
    "        # ==========================================\n",
    "        # B. CATEGORICAL ANALYSIS\n",
    "        # ==========================================\n",
    "        for col in existing_cat:\n",
    "            unique_count = df[col].nunique()\n",
    "            top_val = df[col].mode()[0] if not df[col].mode().empty else \"N/A\"\n",
    "            top_freq = df[col].value_counts().iloc[0] if unique_count > 0 else 0\n",
    "            imbalance = top_freq / len(df)\n",
    "            \n",
    "            categorical_stats_list.append({\n",
    "                'File': filename,\n",
    "                'Feature': col,\n",
    "                'Cardinality': unique_count,\n",
    "                'Top_Class': top_val,\n",
    "                'Imbalance_Ratio': imbalance,\n",
    "                'Total_Rows': len(df)\n",
    "            })\n",
    "    \n",
    "        # ==========================================\n",
    "        # 1. PROTOCOL ANALYSIS\n",
    "        # ==========================================\n",
    "        if 'Protocol' in df.columns:\n",
    "            protocol_map = {6: 'TCP', 17: 'UDP', 1: 'ICMP'}\n",
    "            # Create a temporary column for plotting\n",
    "            df['Protocol_Name'] = df['Protocol'].map(protocol_map).fillna('Other')\n",
    "            \n",
    "            plt.figure(figsize=(8, 5))\n",
    "            sns.countplot(x=df['Protocol_Name'], order=df['Protocol_Name'].value_counts().index)\n",
    "            plt.title(f\"Distribution of Protocols - {filename}\")\n",
    "            plt.show()\n",
    "\n",
    "        # ==========================================\n",
    "        # 2. DESTINATION PORT ANALYSIS\n",
    "        # ==========================================\n",
    "        if 'Destination Port' in df.columns:\n",
    "            top_ports = df['Destination Port'].value_counts().head(10)\n",
    "            \n",
    "            plt.figure(figsize=(10, 5))\n",
    "            sns.barplot(x=top_ports.index, y=top_ports.values, order=top_ports.index)\n",
    "            plt.title(f\"Top 10 Destination Ports - {filename}\")\n",
    "            plt.xlabel(\"Port Number\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.show()\n",
    "\n",
    "        # ==========================================\n",
    "        # 3. FLAG ANALYSIS\n",
    "        # ==========================================\n",
    "        flag_cols = [col for col in df.columns if 'Flag' in col]\n",
    "        if flag_cols:\n",
    "            # Check if flags are numeric before summing\n",
    "            if pd.api.types.is_numeric_dtype(df[flag_cols[0]]):\n",
    "                flag_counts = df[flag_cols].sum().sort_values(ascending=False)\n",
    "                \n",
    "                plt.figure(figsize=(12, 6))\n",
    "                sns.barplot(x=flag_counts.index, y=flag_counts.values)\n",
    "                plt.title(f\"Frequency of Network Flags - {filename}\")\n",
    "                plt.xticks(rotation=45, ha='right')\n",
    "                plt.show()\n",
    "            else:\n",
    "                print(\"Skipping Flag plot: Flag columns appear to be non-numeric.\")\n",
    "\n",
    "        # ==========================================\n",
    "        # 4. VARIANCE SCREENING\n",
    "        # ==========================================\n",
    "        print(f\"\\n--- DETECTING USELESS COLUMNS ({filename}) ---\")\n",
    "        numeric_df = df.select_dtypes(include=[np.number])\n",
    "        std_devs = numeric_df.std()\n",
    "        constant_cols = std_devs[std_devs == 0].index.tolist()\n",
    "        \n",
    "        print(f\"Constant Columns Found: {len(constant_cols)}\")\n",
    "        if constant_cols:\n",
    "            print(constant_cols)\n",
    "\n",
    "        # ==========================================\n",
    "        # 5. CORRELATION ANALYSIS\n",
    "        # ==========================================\n",
    "        print(f\"\\n--- CORRELATION MATRIX ({filename}) ---\")\n",
    "        \n",
    "        # Prepare data for correlation\n",
    "        # Exclude IDs, Labels, Context, and Zero Variance columns\n",
    "        ignore_cols = ['Flow ID', 'Source IP', 'Destination IP', 'Timestamp', 'Label', 'Protocol_Name'] + constant_cols\n",
    "        cols_to_drop = [c for c in ignore_cols if c in df.columns]\n",
    "        \n",
    "        # We only correlate the remaining NUMERIC columns\n",
    "        analysis_df = df.drop(columns=cols_to_drop, errors='ignore').select_dtypes(include=[np.number])\n",
    "        \n",
    "        if not analysis_df.empty:\n",
    "            corr_matrix = analysis_df.corr()\n",
    "\n",
    "            plt.figure(figsize=(16, 12))\n",
    "            sns.heatmap(corr_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "            plt.title(f\"Correlation Matrix - {filename}\")\n",
    "            plt.show()\n",
    "\n",
    "            # Identify High Correlations (> 0.95)\n",
    "            upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "            to_drop_corr = [column for column in upper_tri.columns if any(upper_tri[column].abs() > 0.95)]\n",
    "\n",
    "            print(f\"Number of Highly Correlated (>0.95) Features: {len(to_drop_corr)}\")\n",
    "            print(\"Sample of redundant features:\", to_drop_corr[:10])\n",
    "        else:\n",
    "            print(\"Not enough numeric columns left for correlation analysis.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  [ERROR] {filename}: {e}\")\n",
    "        \n",
    "    del df\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\n--- SANITIZED ANALYSIS COMPLETE ---\")\n",
    "\n",
    "print(\"--- NUMERICAL STATS INSPECTION (First 5 entries per file) ---\")\n",
    "# We use a set to track which files we've already seen to just show a snippet of each\n",
    "seen_files = {}\n",
    "\n",
    "for entry in numeric_stats_list:\n",
    "    fname = entry['File']\n",
    "    \n",
    "    # Initialize counter for this file\n",
    "    if fname not in seen_files:\n",
    "        seen_files[fname] = 0\n",
    "    \n",
    "    # Only show the first 3 features per file to keep output clean\n",
    "    if seen_files[fname] < 3:\n",
    "        print(f\"[{fname}] Feature: {entry['Feature']}\")\n",
    "        print(f\"   Mean: {entry['Mean']:.4f} | Median: {entry['Median']:.4f} | Std: {entry['Std_Dev']:.4f}\")\n",
    "        print(f\"   Skew: {entry['Skewness']:.2f} | Kurt: {entry['Kurtosis']:.2f}\")\n",
    "        print(f\"   Health: Neg={entry['Neg_Values']}, Inf={entry['Inf_Values']}, NaN={entry['NaN_Values']}\")\n",
    "        print(\"-\" * 40)\n",
    "        seen_files[fname] += 1\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "print(\"--- CATEGORICAL STATS INSPECTION (All entries) ---\")\n",
    "for entry in categorical_stats_list:\n",
    "    # We define a warning flag if the data is highly imbalanced (>95%)\n",
    "    warning = \" [IMBALANCED]\" if entry['Imbalance_Ratio'] > 0.95 else \"\"\n",
    "    \n",
    "    print(f\"[{entry['File']}] Feature: {entry['Feature']}\")\n",
    "    print(f\"   Unique Values (Cardinality): {entry['Cardinality']}\")\n",
    "    print(f\"   Most Frequent: '{entry['Top_Class']}' ({entry['Imbalance_Ratio']*100:.2f}% of data){warning}\")\n",
    "    print(\"-\" * 40)\n",
    "# Display Results\n",
    "df_stats_num = pd.DataFrame(numeric_stats_list)\n",
    "df_stats_cat = pd.DataFrame(categorical_stats_list)\n",
    "\n",
    "# CHECK 1: The Init_Win_bytes Mystery\n",
    "print(\"\\n=== NEGATIVE VALUE ANALYSIS ===\")\n",
    "# Show features with high negative counts\n",
    "print(df_stats_num[df_stats_num['Neg_Values'] > 0][['File', 'Feature', 'Min', 'Neg_Values', 'Total_Rows']].sort_values(by='Neg_Values', ascending=False).head(10))\n",
    "\n",
    "# CHECK 2: Distribution Shape (Valid Stats)\n",
    "print(\"\\n=== SKEWNESS REPORT (Cleaned) ===\")\n",
    "print(df_stats_num[['File', 'Feature', 'Skewness', 'Kurtosis']].sort_values(by='Skewness', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe0c2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# CONFIGURATION\n",
    "# ==========================================\n",
    "file_pattern = \"data/*.csv\"  # Update this!\n",
    "files = glob.glob(file_pattern)\n",
    "files.sort()\n",
    "\n",
    "# Sampling Ratio: 10% of the data is usually enough for EDA\n",
    "# If you have < 16GB RAM, lower this to 0.05 (5%)\n",
    "SAMPLE_FRAC = 0.10 \n",
    "\n",
    "global_samples = []\n",
    "\n",
    "print(f\"--- 1. BUILDING GLOBAL STRATIFIED SAMPLE ({SAMPLE_FRAC*100}%) ---\")\n",
    "\n",
    "for filepath in files:\n",
    "    filename = os.path.basename(filepath)\n",
    "    print(f\"Sampling: {filename}...\")\n",
    "    \n",
    "    try:\n",
    "        # Load Raw File\n",
    "        df = pd.read_csv(filepath, low_memory=False)\n",
    "        \n",
    "        # Basic Cleanup (Headers)\n",
    "        df.columns = df.columns.str.strip()\n",
    "        \n",
    "        # STRATIFIED SAMPLING\n",
    "        # We group by Label to ensure we keep a piece of every attack type\n",
    "        # group_keys=False prevents Pandas from adding an extra index\n",
    "        if 'Label' in df.columns:\n",
    "            # We take 10% of EACH class. \n",
    "            # If a class has fewer than 10 rows, we take all of them to preserve rare attacks.\n",
    "            sample = df.groupby('Label', group_keys=False).apply(\n",
    "                lambda x: x.sample(frac=SAMPLE_FRAC) if len(x) > 10 else x\n",
    "            )\n",
    "        else:\n",
    "            # Fallback if no label (should not happen in CIC-IDS2017)\n",
    "            sample = df.sample(frac=SAMPLE_FRAC, random_state=42)\n",
    "            \n",
    "        print(f\"   > Original: {len(df)} rows | Sampled: {len(sample)} rows\")\n",
    "        global_samples.append(sample)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   [ERROR] Could not sample {filename}: {e}\")\n",
    "    \n",
    "    # Memory Cleanup\n",
    "    del df\n",
    "    gc.collect()\n",
    "\n",
    "# ==========================================\n",
    "# 2. AGGREGATING THE GLOBAL DATASET\n",
    "# ==========================================\n",
    "print(\"\\n--- MERGING SAMPLES ---\")\n",
    "global_df = pd.concat(global_samples, axis=0)\n",
    "print(f\"Global Dataset Shape: {global_df.shape}\")\n",
    "print(\"\\nGlobal Label Distribution:\")\n",
    "print(global_df['Label'].value_counts())\n",
    "\n",
    "# ==========================================\n",
    "# 3. GLOBAL DATA CLEANING (For Analysis Only)\n",
    "# ==========================================\n",
    "# We must sanitize the global sample to run Correlation/Variance checks\n",
    "# (We are NOT saving this cleaned version yet, just using it to find bad columns)\n",
    "\n",
    "print(\"\\n--- SANITIZING GLOBAL SAMPLE ---\")\n",
    "\n",
    "# Replace Inf with NaN\n",
    "global_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Separate numeric\n",
    "numeric_df = global_df.select_dtypes(include=[np.number])\n",
    "\n",
    "# ==========================================\n",
    "# 4. GLOBAL VARIANCE ANALYSIS (The \"Drop List\")\n",
    "# ==========================================\n",
    "print(\"\\n--- DETECTING GLOBALLY CONSTANT COLUMNS ---\")\n",
    "# If std() is 0 (or NaN because of all NaNs), the column provides no info\n",
    "std_devs = numeric_df.std()\n",
    "# We look for columns that are effectively zero variance\n",
    "constant_cols = std_devs[std_devs == 0].index.tolist()\n",
    "\n",
    "# Also check for columns that are 100% Null (Empty)\n",
    "null_cols = global_df.columns[global_df.isnull().all()].tolist()\n",
    "cols_to_drop = list(set(constant_cols + null_cols))\n",
    "\n",
    "print(f\"Features with ZERO variance across ALL files: {len(cols_to_drop)}\")\n",
    "print(cols_to_drop)\n",
    "\n",
    "# ==========================================\n",
    "# 5. GLOBAL CORRELATION ANALYSIS\n",
    "# ==========================================\n",
    "print(\"\\n--- GLOBAL CORRELATION ANALYSIS ---\")\n",
    "\n",
    "# Drop the constant columns and Identifiers before correlation\n",
    "ignore_cols = ['Flow ID', 'Source IP', 'Destination IP', 'Timestamp', 'Label'] + cols_to_drop\n",
    "corr_data = global_df.drop(columns=[c for c in ignore_cols if c in global_df.columns])\n",
    "corr_data = corr_data.select_dtypes(include=[np.number])\n",
    "\n",
    "# Compute Matrix\n",
    "corr_matrix = corr_data.corr()\n",
    "\n",
    "# Plot Heatmap\n",
    "plt.figure(figsize=(20, 16))\n",
    "sns.heatmap(corr_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title(\"Global Correlation Matrix (Representative Sample)\")\n",
    "plt.show()\n",
    "\n",
    "# List Pairs > 0.95\n",
    "print(\"\\n--- HIGHLY CORRELATED PAIRS (> 0.95) ---\")\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "# Find index of columns with correlation > 0.95\n",
    "to_drop_corr = [column for column in upper_tri.columns if any(upper_tri[column].abs() > 0.95)]\n",
    "\n",
    "print(f\"Number of Redundant Features: {len(to_drop_corr)}\")\n",
    "print(\"List of Redundant Features (Consider dropping):\")\n",
    "print(to_drop_corr)\n",
    "\n",
    "# ==========================================\n",
    "# 6. GLOBAL DISTRIBUTION CHECK (SKEWNESS)\n",
    "# ==========================================\n",
    "print(\"\\n--- GLOBAL SKEWNESS CHECK ---\")\n",
    "# We check skewness one last time on the global set to confirm Log-Scaling needs\n",
    "skew_series = numeric_df.skew().sort_values(ascending=False)\n",
    "print(\"Top 10 Most Skewed Features (Global):\")\n",
    "print(skew_series.head(10))\n",
    "\n",
    "bad_features = {\n",
    "    \"constant_columns\": cols_to_drop,\n",
    "    \"redundant_correlation\": to_drop_corr\n",
    "}\n",
    "\n",
    "print(\"\\n--- SAVING FEATURE LISTS ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23f3030",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CICIDS_Preprocessor:\n",
    "    def __init__(self, bad_feature_dict, scheme='A', correlation_threshold=0.95):\n",
    "        self.scheme = scheme\n",
    "        self.corr_thresh = correlation_threshold\n",
    "        \n",
    "        # 1. PARSE STATIC DROP LIST\n",
    "        # Combine constant columns and redundant correlation columns into one unique list\n",
    "        self.static_drop_cols = list(set(\n",
    "            bad_feature_dict.get(\"constant_columns\", []) + \n",
    "            bad_feature_dict.get(\"redundant_correlation\", [])\n",
    "        ))\n",
    "        \n",
    "        # Storage for fitted objects\n",
    "        self.scaler = None\n",
    "        self.imputer = None\n",
    "        self.encoders = {}\n",
    "        self.ohe = None\n",
    "        self.feature_selector = None\n",
    "        self.selected_features = None\n",
    "        self.dynamic_drop_cols = [] # Columns found to be correlated during fit()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        print(f\"--- FITTING PREPROCESSOR (SCHEME {self.scheme}) ---\")\n",
    "        # Working copy\n",
    "        X_clean = X.copy()\n",
    "        \n",
    "        # 1. APPLY STATIC DROPS (From your Dictionary)\n",
    "        # Only drop columns that actually exist in this dataframe\n",
    "        existing_drop = [c for c in self.static_drop_cols if c in X_clean.columns]\n",
    "        X_clean.drop(columns=existing_drop, inplace=True)\n",
    "        print(f\"   > Dropped {len(existing_drop)} static bad features.\")\n",
    "        \n",
    "        # 2. CLEAN INFINITY\n",
    "        X_clean.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "        # 3. DYNAMIC CORRELATION CHECK (Safety Net)\n",
    "        # Even though we dropped global redundant cols, the sample might have new correlations\n",
    "        numeric_df = X_clean.select_dtypes(include=[np.number])\n",
    "        corr_matrix = numeric_df.corr().abs()\n",
    "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "        self.dynamic_drop_cols = [column for column in upper.columns if any(upper[column] > self.corr_thresh)]\n",
    "        \n",
    "        if self.dynamic_drop_cols:\n",
    "            print(f\"   > Dropped {len(self.dynamic_drop_cols)} additional correlated features.\")\n",
    "            X_clean.drop(columns=self.dynamic_drop_cols, inplace=True)\n",
    "        \n",
    "        # Identify columns types\n",
    "        cat_cols = ['Protocol'] \n",
    "        if 'Destination Port' in X_clean.columns: cat_cols.append('Destination Port')\n",
    "        # All remaining are numeric\n",
    "        num_cols = [c for c in X_clean.columns if c not in cat_cols]\n",
    "\n",
    "        # 4. IMPUTATION FIT\n",
    "        self.imputer = SimpleImputer(strategy='median')\n",
    "        self.imputer.fit(X_clean[num_cols])\n",
    "        \n",
    "        # 5. SCALING & ENCODING FIT\n",
    "        if self.scheme == 'A':\n",
    "            # RobustScaler\n",
    "            self.scaler = RobustScaler()\n",
    "            self.scaler.fit(X_clean[num_cols])\n",
    "            \n",
    "            # Label Encoding\n",
    "            for col in cat_cols:\n",
    "                le = LabelEncoder()\n",
    "                le.fit(X_clean[col].astype(str))\n",
    "                self.encoders[col] = le\n",
    "                \n",
    "        elif self.scheme == 'B':\n",
    "            # Log1p + StandardScaler\n",
    "            # We transform first to fit the scaler correctly\n",
    "            X_log = np.log1p(X_clean[num_cols].clip(lower=0))\n",
    "            self.scaler = StandardScaler()\n",
    "            self.scaler.fit(X_log)\n",
    "            \n",
    "            # One-Hot Encoding\n",
    "            self.ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "            if 'Protocol' in X_clean.columns:\n",
    "                self.ohe.fit(X_clean[['Protocol']])\n",
    "\n",
    "        # 6. FEATURE SELECTION (Embedded Random Forest)\n",
    "        print(\"   > Running Embedded Feature Selection...\")\n",
    "        # Prepare data for Selector (Basic Imputation + Freq Encoding)\n",
    "        X_temp = X_clean.copy()\n",
    "        X_temp[num_cols] = self.imputer.transform(X_temp[num_cols])\n",
    "        \n",
    "        for col in cat_cols:\n",
    "             freqs = X_temp[col].value_counts()\n",
    "             X_temp[col] = X_temp[col].map(freqs)\n",
    "        \n",
    "        # Train Selector\n",
    "        rf = RandomForestClassifier(n_estimators=50, max_depth=10, random_state=42, n_jobs=-1)\n",
    "        rf.fit(X_temp, y)\n",
    "        \n",
    "        self.feature_selector = SelectFromModel(rf, prefit=True, threshold=\"mean\")\n",
    "        self.selected_features = X_temp.columns[self.feature_selector.get_support()]\n",
    "        print(f\"   > Selected {len(self.selected_features)} features out of {X_temp.shape[1]}\")\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_processed = X.copy()\n",
    "        \n",
    "        # 1. STATIC & DYNAMIC DROPS\n",
    "        # Combine lists and drop\n",
    "        all_drops = list(set(self.static_drop_cols + self.dynamic_drop_cols))\n",
    "        existing_drops = [c for c in all_drops if c in X_processed.columns]\n",
    "        X_processed.drop(columns=existing_drops, inplace=True)\n",
    "        X_processed.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        \n",
    "        # Re-identify columns\n",
    "        cat_cols = ['Protocol']\n",
    "        if 'Destination Port' in X_processed.columns: cat_cols.append('Destination Port')\n",
    "        num_cols = [c for c in X_processed.columns if c not in cat_cols]\n",
    "        \n",
    "        # 2. IMPUTATION\n",
    "        X_processed[num_cols] = self.imputer.transform(X_processed[num_cols])\n",
    "        \n",
    "        # 3. SCHEME TRANSFORMS\n",
    "        if self.scheme == 'A':\n",
    "            X_processed[num_cols] = self.scaler.transform(X_processed[num_cols])\n",
    "            for col in cat_cols:\n",
    "                le = self.encoders.get(col)\n",
    "                # Map unknown labels to the first class (usually 0) to prevent crashing\n",
    "                X_processed[col] = X_processed[col].astype(str).apply(lambda x: x if x in le.classes_ else le.classes_[0])\n",
    "                X_processed[col] = le.transform(X_processed[col])\n",
    "                \n",
    "        elif self.scheme == 'B':\n",
    "            X_processed[num_cols] = np.log1p(X_processed[num_cols].clip(lower=0))\n",
    "            X_processed[num_cols] = self.scaler.transform(X_processed[num_cols])\n",
    "            \n",
    "            if 'Protocol' in X_processed.columns:\n",
    "                proto_ohe = self.ohe.transform(X_processed[['Protocol']])\n",
    "                ohe_cols = [f\"Proto_{i}\" for i in range(proto_ohe.shape[1])]\n",
    "                # Concat and Drop\n",
    "                X_processed = pd.concat([X_processed.reset_index(drop=True), \n",
    "                                         pd.DataFrame(proto_ohe, columns=ohe_cols)], axis=1)\n",
    "                X_processed.drop(columns=['Protocol'], inplace=True)\n",
    "                \n",
    "                # Note: Reset index above is crucial when concatenating with OHE array\n",
    "\n",
    "        # 4. FILTER SELECTED FEATURES\n",
    "        # Only keep columns that the Selector liked\n",
    "        # (We handle the case where OHE might have changed column names slightly or kept them)\n",
    "        # For Scheme A (LabelEnc), names match. For Scheme B, we might lose Protocol, but that's fine.\n",
    "        final_cols = [c for c in self.selected_features if c in X_processed.columns]\n",
    "        \n",
    "        # If Scheme B generated new OHE columns, we usually keep them all or apply selection logic.\n",
    "        # For simplicity in this implementation, we force keep OHE columns if Scheme B\n",
    "        if self.scheme == 'B':\n",
    "            ohe_cols = [c for c in X_processed.columns if 'Proto_' in c]\n",
    "            final_cols.extend(ohe_cols)\n",
    "            final_cols = list(set(final_cols))\n",
    "            \n",
    "        return X_processed[final_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ba4155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 0. SETUP BAD FEATURES LIST\n",
    "# ==========================================\n",
    "# Ensure your dictionary is defined here (from previous steps)\n",
    "# bad_features = { \"constant_columns\": [...], \"redundant_correlation\": [...] }\n",
    "# If you don't have the variable in memory, stick empty lists to avoid crash:\n",
    "if 'bad_features' not in locals():\n",
    "    print(\"Warning: bad_features dictionary not found. Creating empty placeholder.\")\n",
    "    bad_features = {\"constant_columns\": [], \"redundant_correlation\": []}\n",
    "\n",
    "# ==========================================\n",
    "# 1. PREPARE X AND y\n",
    "# ==========================================\n",
    "print(\"--- 1. SEPARATING X AND y ---\")\n",
    "identifiers = ['Flow ID', 'Source IP', 'Destination IP', 'Timestamp']\n",
    "X = global_df.drop(columns=['Label'] + [c for c in identifiers if c in global_df.columns])\n",
    "y = global_df['Label']\n",
    "\n",
    "# ==========================================\n",
    "# 2. SAFE STRATIFIED SPLIT\n",
    "# ==========================================\n",
    "print(\"\\n--- 2. HANDLING RARE CLASSES & SPLITTING ---\")\n",
    "\n",
    "# Identify classes with only 1 sample\n",
    "class_counts = y.value_counts()\n",
    "rare_classes = class_counts[class_counts < 2].index.tolist()\n",
    "\n",
    "if len(rare_classes) > 0:\n",
    "    print(f\"   > Detected rare classes (1 sample): {rare_classes}\")\n",
    "    print(\"   > Removing them from split and forcing them into Train set...\")\n",
    "\n",
    "# Create mask\n",
    "mask_rare = y.isin(rare_classes)\n",
    "\n",
    "# Separate the singletons\n",
    "X_rare = X[mask_rare]\n",
    "y_rare = y[mask_rare]\n",
    "\n",
    "# The rest of the data (Safe to split)\n",
    "X_common = X[~mask_rare]\n",
    "y_common = y[~mask_rare]\n",
    "\n",
    "# Split the common data\n",
    "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(\n",
    "    X_common, y_common, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y_common\n",
    ")\n",
    "\n",
    "# Merge Rare data back into Training Set ONLY\n",
    "X_train = pd.concat([X_train_c, X_rare])\n",
    "y_train = pd.concat([y_train_c, y_rare])\n",
    "\n",
    "# Test set is just the common test split\n",
    "X_test = X_test_c\n",
    "y_test = y_test_c\n",
    "\n",
    "print(f\"   > Training Data: {X_train.shape}\")\n",
    "print(f\"   > Testing Data:  {X_test.shape}\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. APPLY PREPROCESSOR\n",
    "# ==========================================\n",
    "print(\"\\n--- 3. FITTING PREPROCESSOR ---\")\n",
    "\n",
    "# Initialize with the Bad Features Dictionary\n",
    "preprocessor = CICIDS_Preprocessor(bad_feature_dict=bad_features, scheme='A')\n",
    "\n",
    "# Fit on Train\n",
    "preprocessor.fit(X_train, y_train)\n",
    "\n",
    "# Transform\n",
    "print(\"\\n--- 4. TRANSFORMING ---\")\n",
    "X_train_processed = preprocessor.transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print(f\"Final Train Shape: {X_train_processed.shape}\")\n",
    "print(f\"Final Test Shape:  {X_test_processed.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
